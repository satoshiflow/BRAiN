# BRAiN Deployment Fix & Task Completion Plan

## Current Situation

**Problem:** Deployment timed out, containers still running OLD code (Up ~1 hour)
- Frontend ENV variables not applied: `NEXT_PUBLIC_BRAIN_API_BASE` still points to `http://backend:8000`
- Browser shows API calls going to wrong URL: `control.dev.brain.falklabs.de` instead of `api.dev.brain.falklabs.de`
- All changes committed to GitHub (commit 69bd8b2) but not deployed

**Evidence:**
```bash
docker ps ‚Üí Shows containers "Up About an hour"
Browser Network Tab ‚Üí Shows API calls to control.dev.brain.falklabs.de (wrong!)
```

**Files Modified (not yet active):**
- `/home/user/BRAiN/docker-compose.yml` - Frontend ENV: `https://api.dev.brain.falklabs.de`
- `/home/user/BRAiN/backend/.env` - CORS: restricted domains, REDIS_URL: service name, ENVIRONMENT: production

## Task Sequence (User Requested Order)

### Task A: Manual Container Redeploy + Test CORS Fix (IMMEDIATE)

**Goal:** Get new ENV variables active in containers

**CRITICAL DISCOVERY (2026-01-15 17:34):**
- ‚ùå Coolify Deployment l√§uft in Timeout (2x versucht)
- ‚ùå Container laufen weiter als ALTE Versionen ("Up 2 hours")
- ‚ùå Frontend hat Gateway Timeout (504) - Control Deck & AXE UI
- ‚úÖ Backend funktioniert (HTTP 200)
- ‚úÖ Test-Suite: 14/16 bestanden (87%)

**Root Cause:** Container m√ºssen MANUELL gestoppt werden BEVOR Coolify Deploy getriggert wird.

**UPDATED Steps (CORRECT Order):**

#### Option A: Via SSH (Empfohlen f√ºr schnelle Ausf√ºhrung)

1. **Stop and remove old containers FIRST**
   ```bash
   ssh root@brain.falklabs.de

   # Container IDs vom aktuellen Stand (Up 2 hours):
   docker stop backend-mw0ck04s8go048c0g4so48cc-163431677848 \
               control_deck-mw0ck04s8go048c0g4so48cc-163431692672 \
               axe_ui-mw0ck04s8go048c0g4so48cc-163431704628

   docker rm backend-mw0ck04s8go048c0g4so48cc-163431677848 \
             control_deck-mw0ck04s8go048c0g4so48cc-163431692672 \
             axe_ui-mw0ck04s8go048c0g4so48cc-163431704628
   ```

2. **Verify containers are gone**
   ```bash
   docker ps --filter "name=mw0ck04s8go048c0g4so48cc" --filter "name=backend" --format "table {{.Names}}\t{{.Status}}"
   docker ps --filter "name=mw0ck04s8go048c0g4so48cc" --filter "name=control_deck" --format "table {{.Names}}\t{{.Status}}"
   docker ps --filter "name=mw0ck04s8go048c0g4so48cc" --filter "name=axe_ui" --format "table {{.Names}}\t{{.Status}}"
   # Should show NO results for these three
   ```

3. **THEN Trigger Force Deploy in Coolify UI**
   - Coolify ‚Üí BRAiN Project ‚Üí "Force Deploy" button
   - **Monitor logs closely** - build should start immediately
   - Wait for "Deployment successful" message

#### Option B: Via Coolify UI (Alternative)

1. **Stop containers in Coolify UI first**
   - Coolify ‚Üí BRAiN Project ‚Üí Backend ‚Üí Stop
   - Coolify ‚Üí BRAiN Project ‚Üí Control Deck ‚Üí Stop
   - Coolify ‚Üí BRAiN Project ‚Üí AXE UI ‚Üí Stop

2. **Delete stopped containers** (optional, Coolify might do this)
   - Via SSH: `docker rm <container-names>`

3. **Trigger Force Deploy**
   - Coolify ‚Üí BRAiN Project ‚Üí "Force Deploy"

#### Post-Deployment Verification

4. **Verify Deployment**
   ```bash
   # Check container ages (MUST show "Up < 5 minutes" - fresh!)
   docker ps --filter "name=mw0ck04s8go048c0g4so48cc" --format "table {{.Names}}\t{{.Status}}"

   # Run test suite
   ~/brain-test-v2.sh
   # Expected: 16/16 tests passed (100%)
   ```

5. **Test CORS Fix in Browser**
   - Open Control Deck: https://control.dev.brain.falklabs.de
   - Open DevTools ‚Üí Network tab
   - **EXPECTED:** API calls go to `https://api.dev.brain.falklabs.de/...` (NOT control.dev.brain.falklabs.de!)
   - **CHECK:** Response headers show correct CORS headers
   - **VERIFY:** No 404 errors for API endpoints

**Success Criteria:**
- ‚úÖ Containers show "Up < 5 minutes" status
- ‚úÖ Browser Network tab shows API calls to `api.dev.brain.falklabs.de`
- ‚úÖ No CORS errors in browser console
- ‚úÖ Control Deck loads data successfully
- ‚úÖ AXE UI loads successfully (no 504)
- ‚úÖ Test suite: 16/16 passed (100%)

**Why This Happens:**
Coolify deployment timeout likely occurs because:
1. Build process takes too long (Next.js builds can be slow)
2. Old containers hold locks on resources
3. Coolify's build timeout is too aggressive for cold builds

**Manual container removal forces Coolify to do a clean rebuild.**

---

### Task B: Setup Coolify Auto-Deploy Webhook (AFTER Task A)

**Goal:** GitHub PR merge to v2 ‚Üí automatic Coolify deployment

**Current State:** PRs auto-merge but Coolify requires manual "Deploy All" click

**Steps:**
1. **Get Coolify Webhook URL**
   - Coolify UI ‚Üí BRAiN Project ‚Üí Settings ‚Üí Webhooks
   - Copy webhook URL (format: `https://brain.falklabs.de/api/v1/webhooks/...`)

2. **Configure GitHub Webhook**
   - GitHub ‚Üí satoshiflow/BRAiN ‚Üí Settings ‚Üí Webhooks ‚Üí Add webhook
   - Payload URL: Coolify webhook URL
   - Content type: application/json
   - Events: "Just the push event" (or "Pull request" for PR merges)
   - Active: ‚úì

3. **Test Webhook**
   - Make small commit to v2 branch
   - Check Coolify deployment logs for automatic trigger
   - Verify deployment completes successfully

**Success Criteria:**
- ‚úÖ Webhook appears in GitHub settings
- ‚úÖ Test push triggers Coolify deployment automatically
- ‚úÖ No manual "Deploy All" clicks required

---

### Task C: Complete Genesis/Foundation Modules (IN PROGRESS)

**Context:** User requested: "Genesis/Foundation Module vervollst√§ndigen" during deployment

**Exploration Results:**

#### üéâ Genesis Module: ‚úÖ FULLY COMPLETE (v1.0.0 + v2.0.0)

**Locations:**
- Core System: `/home/user/BRAiN/backend/app/modules/genesis/` (v1.0.0)
- Agent System: `/home/user/BRAiN/backend/brain/agents/genesis_agent/` (v2.0.0)

**Status:** Production-ready with:
- ‚úÖ 8 API endpoints (spawn, evolve, reproduce, validate, blueprints, traits)
- ‚úÖ 26 trait definitions across 6 categories
- ‚úÖ 5 built-in blueprints (fleet, safety, navigation, code, ops)
- ‚úÖ Ethics & safety validation (Foundation Layer integration)
- ‚úÖ DNA integration with mutation tracking
- ‚úÖ Comprehensive tests (test_genesis.py, test_genesis_agent.py)
- ‚úÖ Full documentation (README.md + DESIGN.md)
- ‚úÖ Database migrations (005_genesis)

**Conclusion:** NO WORK NEEDED - Genesis is complete!

---

#### ‚ö†Ô∏è Foundation Module: PARTIAL - 3 Missing Endpoints

**Location:** `/home/user/BRAiN/backend/app/modules/foundation/`

**Current State:**
- ‚úÖ 8 API endpoints implemented
- ‚úÖ 26 pytest tests (good coverage)
- ‚úÖ Comprehensive documentation (README.md)
- ‚úÖ Core validation logic working
- ‚úÖ Behavior tree structure (placeholder)

**Missing Components (per CLAUDE.md API Reference):**

| Expected Endpoint | Current Status | Impact |
|-------------------|----------------|--------|
| `/api/foundation/info` | ‚ùå Missing | CLAUDE.md expects this for system info |
| `/api/foundation/authorize` | ‚ùå Missing | Authorization checks not implemented |
| `/api/foundation/audit-log` | ‚ùå Missing | No persistent audit trail |

**Current Endpoints (working):**
- `/api/foundation/status` - Metrics (similar to missing `info`)
- `/api/foundation/config` - GET/PUT configuration
- `/api/foundation/validate` - Action validation (ethics/safety)
- `/api/foundation/validate-batch` - Batch validation
- `/api/foundation/behavior-tree/execute` - Tree execution (placeholder)
- `/api/foundation/behavior-tree/validate` - Tree validation
- `/api/foundation/health` - Health check

---

#### Implementation Plan: Complete Foundation Module

**Goal:** Add 3 missing endpoints to match CLAUDE.md specification

##### 1. Add `/api/foundation/info` Endpoint

**Purpose:** Return Foundation system information (name, version, capabilities)

**Implementation:**
```python
# router.py
@router.get("/info", response_model=FoundationInfo)
async def get_foundation_info():
    """Get Foundation system information."""
    return FoundationInfo(
        name="BRAiN Foundation Layer",
        version="1.0.0",
        capabilities=[
            "action_validation",
            "ethics_rules",
            "safety_patterns",
            "behavior_trees"
        ],
        status="operational",
        uptime=foundation_service.get_uptime()
    )
```

**Schema:**
```python
# schemas.py
class FoundationInfo(BaseModel):
    name: str
    version: str
    capabilities: List[str]
    status: str
    uptime: float
```

**Files to modify:**
- `backend/app/modules/foundation/schemas.py` - Add FoundationInfo model
- `backend/app/modules/foundation/router.py` - Add GET /info endpoint
- `backend/app/modules/foundation/service.py` - Add get_uptime() method

##### 2. Add `/api/foundation/authorize` Endpoint

**Purpose:** Check action authorization (different from ethics validation)

**Design Decision:**
- `/validate` = Ethics/safety check (is this ethical/safe?)
- `/authorize` = Permission check (does this agent have permission?)

**Implementation:**
```python
# schemas.py
class AuthorizationRequest(BaseModel):
    agent_id: str
    action: str
    resource: str
    context: Dict[str, Any] = Field(default_factory=dict)

class AuthorizationResponse(BaseModel):
    authorized: bool
    reason: str
    required_permissions: List[str] = Field(default_factory=list)
    audit_id: str

# service.py
def authorize_action(self, request: AuthorizationRequest) -> AuthorizationResponse:
    """Check if agent is authorized for action."""
    # TODO: Integrate with Policy Engine for permission checks
    # For now, basic implementation:

    # Check if action is in blacklist
    if request.action in self.config.blocked_actions:
        return AuthorizationResponse(
            authorized=False,
            reason=f"Action '{request.action}' is globally blocked",
            required_permissions=[],
            audit_id=self._generate_audit_id()
        )

    # Placeholder for real permission system
    # Future: Check agent roles, resource ACLs, policy engine
    return AuthorizationResponse(
        authorized=True,
        reason="Agent has required permissions",
        required_permissions=[],
        audit_id=self._generate_audit_id()
    )

# router.py
@router.post("/authorize", response_model=AuthorizationResponse)
async def authorize_action(request: AuthorizationRequest):
    """Check action authorization."""
    return foundation_service.authorize_action(request)
```

**Files to modify:**
- `backend/app/modules/foundation/schemas.py` - Add AuthorizationRequest/Response
- `backend/app/modules/foundation/service.py` - Add authorize_action() method
- `backend/app/modules/foundation/router.py` - Add POST /authorize endpoint

##### 3. Add `/api/foundation/audit-log` Endpoint

**Purpose:** Retrieve audit trail of validations and authorizations

**Design:**
- Store validation/authorization events in-memory or database
- Query with filters (agent_id, action, timestamp range, outcome)
- Paginated results

**Implementation:**
```python
# schemas.py
class AuditLogEntry(BaseModel):
    audit_id: str
    timestamp: datetime
    event_type: str  # "validation" or "authorization"
    agent_id: Optional[str] = None
    action: str
    outcome: str  # "allowed", "blocked", "authorized", "denied"
    reason: str
    details: Dict[str, Any] = Field(default_factory=dict)

class AuditLogRequest(BaseModel):
    agent_id: Optional[str] = None
    action: Optional[str] = None
    event_type: Optional[str] = None
    outcome: Optional[str] = None
    limit: int = Field(default=100, ge=1, le=1000)
    offset: int = Field(default=0, ge=0)

class AuditLogResponse(BaseModel):
    entries: List[AuditLogEntry]
    total: int
    limit: int
    offset: int

# service.py
class FoundationService:
    def __init__(self):
        # ... existing code ...
        self.audit_log: List[AuditLogEntry] = []  # In-memory for now
        self.max_audit_entries = 10000

    def _log_audit(self, event_type: str, agent_id: Optional[str],
                   action: str, outcome: str, reason: str, details: Dict):
        """Log audit event."""
        entry = AuditLogEntry(
            audit_id=self._generate_audit_id(),
            timestamp=datetime.now(),
            event_type=event_type,
            agent_id=agent_id,
            action=action,
            outcome=outcome,
            reason=reason,
            details=details
        )

        self.audit_log.append(entry)

        # Keep only last N entries
        if len(self.audit_log) > self.max_audit_entries:
            self.audit_log = self.audit_log[-self.max_audit_entries:]

    def query_audit_log(self, request: AuditLogRequest) -> AuditLogResponse:
        """Query audit log with filters."""
        # Filter entries
        entries = self.audit_log

        if request.agent_id:
            entries = [e for e in entries if e.agent_id == request.agent_id]
        if request.action:
            entries = [e for e in entries if e.action == request.action]
        if request.event_type:
            entries = [e for e in entries if e.event_type == request.event_type]
        if request.outcome:
            entries = [e for e in entries if e.outcome == request.outcome]

        # Sort by timestamp descending (newest first)
        entries = sorted(entries, key=lambda e: e.timestamp, reverse=True)

        # Paginate
        total = len(entries)
        paginated = entries[request.offset:request.offset + request.limit]

        return AuditLogResponse(
            entries=paginated,
            total=total,
            limit=request.limit,
            offset=request.offset
        )

# router.py
@router.get("/audit-log", response_model=AuditLogResponse)
async def get_audit_log(
    agent_id: Optional[str] = Query(None),
    action: Optional[str] = Query(None),
    event_type: Optional[str] = Query(None),
    outcome: Optional[str] = Query(None),
    limit: int = Query(100, ge=1, le=1000),
    offset: int = Query(0, ge=0)
):
    """Retrieve audit log."""
    request = AuditLogRequest(
        agent_id=agent_id,
        action=action,
        event_type=event_type,
        outcome=outcome,
        limit=limit,
        offset=offset
    )
    return foundation_service.query_audit_log(request)
```

**Integration Points:**
- Update `validate_action()` to call `_log_audit()` after validation
- Update `authorize_action()` to call `_log_audit()` after authorization
- Future: Store to PostgreSQL instead of in-memory

**Files to modify:**
- `backend/app/modules/foundation/schemas.py` - Add AuditLog models
- `backend/app/modules/foundation/service.py` - Add audit logging
- `backend/app/modules/foundation/router.py` - Add GET /audit-log endpoint

---

#### Testing Plan

Add tests for new endpoints:

```python
# tests/test_foundation.py

def test_get_foundation_info(client):
    """Test GET /api/foundation/info endpoint."""
    response = client.get("/api/foundation/info")
    assert response.status_code == 200
    data = response.json()
    assert data["name"] == "BRAiN Foundation Layer"
    assert "capabilities" in data

def test_authorize_action_allowed(client):
    """Test POST /api/foundation/authorize - allowed action."""
    response = client.post("/api/foundation/authorize", json={
        "agent_id": "test_agent",
        "action": "read_file",
        "resource": "/data/test.txt"
    })
    assert response.status_code == 200
    data = response.json()
    assert data["authorized"] is True

def test_authorize_action_blocked(client):
    """Test POST /api/foundation/authorize - blocked action."""
    response = client.post("/api/foundation/authorize", json={
        "agent_id": "test_agent",
        "action": "delete_all",
        "resource": "/data"
    })
    assert response.status_code == 200
    data = response.json()
    assert data["authorized"] is False

def test_audit_log_query(client):
    """Test GET /api/foundation/audit-log endpoint."""
    # First, create some audit entries via validation
    client.post("/api/foundation/validate", json={
        "agent_id": "test_agent",
        "action": "test_action",
        "context": {}
    })

    # Query audit log
    response = client.get("/api/foundation/audit-log?limit=10")
    assert response.status_code == 200
    data = response.json()
    assert "entries" in data
    assert data["total"] >= 1
```

---

#### Documentation Updates

**Update CLAUDE.md API Reference:**

Replace Foundation API section with correct endpoints:

```markdown
### Foundation Layer (`/api/foundation`) üÜï

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/api/foundation/info` | Foundation system information |
| GET | `/api/foundation/status` | System status & metrics |
| GET | `/api/foundation/config` | Get configuration |
| PUT | `/api/foundation/config` | Update configuration |
| POST | `/api/foundation/validate` | Validate action (ethics/safety) |
| POST | `/api/foundation/validate-batch` | Batch validation |
| POST | `/api/foundation/authorize` | Check action authorization |
| GET | `/api/foundation/audit-log` | Retrieve audit trail |
| POST | `/api/foundation/behavior-tree/execute` | Execute behavior tree |
| POST | `/api/foundation/behavior-tree/validate` | Validate behavior tree |
| GET | `/api/foundation/health` | Health check |
```

**Add example usage to Foundation README.md:**

```markdown
### Authorization Check

Check if an agent is authorized to perform an action:

\`\`\`python
from backend.app.modules.foundation.service import get_foundation_service
from backend.app.modules.foundation.schemas import AuthorizationRequest

foundation = get_foundation_service()

result = foundation.authorize_action(AuthorizationRequest(
    agent_id="ops_agent",
    action="deploy_to_production",
    resource="brain-backend",
    context={"environment": "production"}
))

if not result.authorized:
    print(f"Unauthorized: {result.reason}")
\`\`\`

### Audit Log Query

Retrieve recent validation/authorization events:

\`\`\`python
from backend.app.modules.foundation.schemas import AuditLogRequest

result = foundation.query_audit_log(AuditLogRequest(
    agent_id="ops_agent",
    outcome="blocked",
    limit=50
))

for entry in result.entries:
    print(f"{entry.timestamp}: {entry.action} - {entry.outcome}")
\`\`\`
```

---

#### Implementation Summary

**Files to Create/Modify:**

1. **`backend/app/modules/foundation/schemas.py`**
   - Add FoundationInfo
   - Add AuthorizationRequest/Response
   - Add AuditLogEntry, AuditLogRequest, AuditLogResponse

2. **`backend/app/modules/foundation/service.py`**
   - Add get_uptime() method
   - Add authorize_action() method
   - Add audit_log list + _log_audit() method
   - Add query_audit_log() method
   - Update validate_action() to log audit entries

3. **`backend/app/modules/foundation/router.py`**
   - Add GET /info endpoint
   - Add POST /authorize endpoint
   - Add GET /audit-log endpoint

4. **`backend/tests/test_foundation.py`**
   - Add test_get_foundation_info()
   - Add test_authorize_action_allowed()
   - Add test_authorize_action_blocked()
   - Add test_audit_log_query()

5. **`backend/app/modules/foundation/README.md`**
   - Add authorization usage example
   - Add audit log usage example

6. **`/home/user/BRAiN/CLAUDE.md`**
   - Update Foundation API Reference section

**Estimated Effort:** 2-3 hours (straightforward additions to existing module)

**Risk Level:** LOW (all changes are additive, no breaking changes)

---

#### Deployment Impact

**Changes Required:**
- ‚úÖ Backend code changes only
- ‚úÖ No database migrations needed (in-memory audit for now)
- ‚úÖ No frontend changes required
- ‚úÖ Backward compatible (new endpoints, existing ones unchanged)

**Testing:**
- Run pytest: `pytest backend/tests/test_foundation.py -v`
- Manual API testing: Use Swagger UI at `/docs`

**Rollout:**
1. Implement changes locally
2. Run tests
3. Commit to branch
4. PR to v2
5. Deploy via Coolify (after Task A completes)

---

## üî¥ CRITICAL ISSUE IDENTIFIED (2026-01-15 20:12)

**Problem:** Gateway Timeout (504) auf ALLE Services nach CORS-√Ñnderung

**Root Cause Analysis:**
- Container laufen ("Up 14 minutes" - NEU gebaut!)
- Direkte Verbindung funktioniert (Phase 5: ‚úÖ)
- Backend startet erfolgreich ("Application startup complete")
- **ABER:** Traefik Health Checks schlagen fehl!

**Fehler in `backend/.env`:**
```bash
# AKTUELL (FALSCH):
CORS_ORIGINS=["https://api.dev.brain.falklabs.de","https://control.dev.brain.falklabs.de","https://axe.dev.brain.falklabs.de"]

# PROBLEM: Traefik macht Health Checks √ºber http://localhost:8000
# Diese Requests werden jetzt von CORS blockiert!
```

**L√∂sung:**
```bash
# RICHTIG:
CORS_ORIGINS=["https://api.dev.brain.falklabs.de","https://control.dev.brain.falklabs.de","https://axe.dev.brain.falklabs.de","http://localhost:3000","http://127.0.0.1:3000","http://10.0.39.7:8000"]
```

**Deployment-Status:**
- ‚úÖ Container wurden NEU gebaut (14 Minuten alt)
- ‚úÖ Alle Services starten erfolgreich
- ‚ùå Traefik kann nicht mit ihnen kommunizieren (CORS-Block)
- `/home/user/BRAiN/brain-test-improved.sh` - Deployment test suite (16 tests)

### Documentation:
- `/home/user/BRAiN/CLAUDE.md` - "Coolify Deployment Best Practices" section

---

## Risk Mitigation

**Backup Created:** Hetzner snapshot "brain-production-working-2026-01-15" (69.3 GB)
- Snapshot taken before current deployment attempt
- Can rollback if issues arise

**Rollback Procedure:**
1. Restore Hetzner snapshot
2. Reboot server
3. All services return to working state

---

## ‚úÖ ERFOLG - ALLE TESTS BESTANDEN (2026-01-15 21:15)

### Task A: Deployment & CORS-Fix ‚úÖ ABGESCHLOSSEN
- Branch `claude/review-and-plan-TtwLT` in `v2` gemerged
- Coolify Proxy Server restartet (Traefik Cache geleert)
- **Test-Ergebnis: 16/16 Tests bestanden (100%)**
  - ‚úÖ Backend API: HTTP 200
  - ‚úÖ Control Deck: HTTP 307 (Redirect normal)
  - ‚úÖ AXE UI: HTTP 200
  - ‚úÖ Alle Container laufen (Up 55 minutes)
  - ‚úÖ Traefik Labels korrekt
  - ‚úÖ DNS-Aufl√∂sung funktioniert
  - ‚úÖ Direkte Container-Verbindung funktioniert

### Task C: Foundation Module ‚úÖ ABGESCHLOSSEN
- ‚úÖ C.4: 4 neue pytest Tests hinzugef√ºgt (+117 Zeilen)
- ‚úÖ C.5: Foundation README.md mit Beispielen aktualisiert (+48 Zeilen)
- ‚úÖ C.6: CLAUDE.md API-Referenz korrigiert (+43 Zeilen)
- Commit: `cfd8df1` - "docs: Complete Foundation module documentation and tests (C.4-C.6)"
- Branch in `v2` gemerged

**L√∂sung f√ºr Gateway Timeout:**
- Problem war Traefik Service Discovery Cache
- L√∂sung: Coolify Proxy restart ‚Üí Cache gel√∂scht
- Keine Code-√Ñnderungen notwendig (CORS war bereits korrekt)

---

## Execution Order

1. ~~**Task A - Manual redeploy + CORS test**~~ ‚úÖ ERLEDIGT (100% Tests)
2. **NEXT:** Task B - Coolify auto-deploy webhook ‚è≥ BEREIT
3. ~~**Task C - Genesis/Foundation completion**~~ ‚úÖ ERLEDIGT (208 Zeilen Doku)

User explicitly requested: "dann genau in der Reihenfolge A, B und C abarbeiten"
User confirmed: "ok weiter" ‚Üí Bereit f√ºr Task B

---

## üöÄ Task B: Coolify Auto-Deploy Webhook - PLAN

### Ziel
GitHub Webhook konfigurieren, sodass jeder PR merge in `v2` automatisch ein Coolify-Deployment triggert (kein manueller "Deploy All" Button mehr).

### Voraussetzungen
- ‚úÖ BRAiN l√§uft auf Coolify (brain.falklabs.de:8000)
- ‚úÖ Projekt UUID: `mw0ck04s8go048c0g4so48cc`
- ‚úÖ GitHub Repo: `satoshiflow/BRAiN`
- ‚úÖ Branch: `v2` (main deployment branch)

### Schritt 1: Coolify Webhook URL finden (SSH auf Server)

**Option A: Via Coolify API (Empfohlen)**

```bash
# SSH auf Server
ssh root@brain.falklabs.de

# 1. Finde Coolify Port (sollte 8000 sein laut Docs)
docker ps | grep coolify | grep -oP '\d+:8000'

# 2. Teste Coolify API Zugriff
curl -s http://localhost:8000/api/v1/healthcheck

# 3. Finde Webhook URL via Docker Inspect
docker inspect $(docker ps -q --filter "name=backend-mw0ck04s8go048c0g4so48cc") \
  | grep -A 5 "COOLIFY_WEBHOOK_URL" || echo "Webhook URL nicht in ENV"

# 4. Alternative: Suche in Coolify Container Logs
docker logs $(docker ps -q --filter "name=coolify") 2>&1 \
  | grep -i "webhook" | tail -10
```

**Option B: Via Coolify UI (Fallback)**

1. √ñffne: https://brain.falklabs.de (oder Port 8000)
2. Login mit deinem Account
3. Navigiere zu: **BRAiN Project** ‚Üí **Settings** ‚Üí **Webhooks** (oder **General**)
4. Kopiere die "Webhook URL" - Format:
   ```
   https://brain.falklabs.de/api/v1/deploy-webhook/<uuid>
   ```

**Erwartetes Ergebnis:**
```
https://brain.falklabs.de/api/v1/deploy-webhook/mw0ck04s8go048c0g4so48cc
```

### Schritt 2: GitHub Webhook einrichten

1. **GitHub Repo Settings:**
   - Gehe zu: https://github.com/satoshiflow/BRAiN/settings/hooks
   - Klicke: "Add webhook"

2. **Webhook Konfiguration:**
   ```
   Payload URL: https://brain.falklabs.de/api/v1/deploy-webhook/mw0ck04s8go048c0g4so48cc
   Content type: application/json
   Secret: (leer lassen oder Coolify Secret verwenden)
   SSL verification: Enable SSL verification

   Which events would you like to trigger this webhook?
   ‚úÖ Just the push event
   OR
   ‚úÖ Let me select individual events
      - [x] Pull requests (merged)
      - [x] Pushes

   Active: ‚úÖ Yes
   ```

3. **Save webhook**

### Schritt 3: Testen mit Dummy-Commit

**Testplan:**

1. **Erstelle Test-Branch:**
   ```bash
   git checkout v2
   git pull origin v2
   git checkout -b test/webhook-trigger-$(date +%s)
   ```

2. **Dummy-Commit:**
   ```bash
   echo "# Webhook Test $(date)" >> /tmp/webhook_test.txt
   git add /tmp/webhook_test.txt
   git commit -m "test: Trigger Coolify auto-deploy webhook test"
   git push -u origin HEAD
   ```

3. **GitHub PR erstellen & mergen:**
   - Erstelle PR: test-Branch ‚Üí v2
   - Merge PR
   - **Erwartung:** Coolify startet automatisch Deployment

4. **Verifizierung:**
   ```bash
   # Auf Server: Logs pr√ºfen
   ssh root@brain.falklabs.de
   docker logs $(docker ps -q --filter "name=backend-mw0ck04s8go048c0g4so48cc") --tail 50

   # Timestamp vergleichen: Sollte nach PR merge neu sein
   ```

5. **GitHub Webhook Logs pr√ºfen:**
   - Gehe zu: https://github.com/satoshiflow/BRAiN/settings/hooks
   - Klicke auf den Webhook
   - Tab: "Recent Deliveries"
   - Pr√ºfe Response: Sollte `200 OK` sein

### Schritt 4: Cleanup (nach erfolgreichem Test)

```bash
# Test-Datei entfernen
git checkout v2
git pull origin v2
rm /tmp/webhook_test.txt
git add /tmp/webhook_test.txt
git commit -m "chore: Remove webhook test file"
git push origin v2

# Test-Branch l√∂schen
git push origin --delete test/webhook-trigger-*
git branch -D test/webhook-trigger-*
```

### Erwartetes Ergebnis

‚úÖ **Erfolg, wenn:**
- GitHub zeigt Webhook Delivery mit `200 OK`
- Coolify startet automatisch Deployment nach PR merge
- Container werden neu gebaut (neuer Timestamp in `docker ps`)
- Test-Suite l√§uft: `~/brain-test-v2.sh` zeigt 16/16 Tests

### Troubleshooting

**Problem: Webhook liefert 404/401/403**
- Webhook URL √ºberpr√ºfen (UUID korrekt?)
- Coolify Secret in GitHub Webhook eintragen
- Coolify Logs pr√ºfen: `docker logs coolify-proxy`

**Problem: Deployment startet nicht**
- Coolify UI: "Deployment History" pr√ºfen
- Branch-Trigger konfiguriert? (nur `v2` oder `*`)
- GitHub Webhook Events: "Pushes" aktiviert?

**Problem: Deployment schl√§gt fehl**
- Siehe vorheriger Task A Troubleshooting
- CORS Settings in `.env` korrekt?
- Traefik Proxy restart falls n√∂tig

### Dateien zum Commiten (nach Task B)

- `/root/.claude/plans/sunny-sleeping-zebra.md` (dieser Plan)
- (Keine Code-√Ñnderungen n√∂tig - nur GitHub/Coolify Config)

### Zeitsch√§tzung
- Webhook URL finden: 5 Min
- GitHub Webhook setup: 5 Min
- Test-Commit & Verifikation: 10 Min
- **Total: ~20 Min**

---

## üé® TASK D: Control Deck UI Improvements (2026-01-15 21:45)

### User Requirements

**A. Design-Referenz:**
- **Inspiration:** https://shadcnblocks-admin.vercel.app/
- **Stil:** Modern, sleek, professional admin dashboard (wie Freepik "sleek-admin-dashboard")
- **Komponenten:** shadcn/ui based (bereits vorhanden)
- **Ziel:** Optisch ansprechender, besser organisiert

**B. Strukturelle √Ñnderungen:**
```
Aktuell: Flache Navigation mit 14 Gruppen, 37 Pages
Neu: 3 logische Hauptbereiche mit Collapsible Sections
```

**1. BRAiN Einstellungen** (Settings & Configuration) - Icon: Settings2, Color: Blue
- System Settings, API Config, Identity & Access
- LLM Configuration, Policy Engine, Credits System
- Core Modules

**2. Monitoring & √úberwachung** (Monitoring & Surveillance) - Icon: Activity, Color: Green
- System Dashboard, Health, Missions Overview/History
- Agent Management, Supervisor Panel, Telemetry
- Hardware Resources, Immune System, Threat Events
- NeuroRail Trace/Health Matrix, Fleet Management

**3. Tools/Desktop** (Productivity & Automation) - Icon: Wrench, Color: Purple
- Course Factory, Business Factory
- WebGenesis Sites, Create New Site
- Create Agent, Constitutional Agents
- DNA Evolution, Knowledge Graph

**C. Priorisierte Aufgaben (vom User best√§tigt):**

**P1 - Quick Win (30 Min):**
- API Config Fix: `NEXT_PUBLIC_BRAIN_API_URL` ‚Üí `NEXT_PUBLIC_BRAIN_API_BASE`
- 5 Dateien: dashboardApi, neurorailApi, coreOverviewApi, missionsApi, agentsApi

**P2 - Real-time Updates (2 Std):**
- WebSocket f√ºr Missions, Health, Telemetry
- Hooks: `useMissionWebSocket.ts`, `useHealthSSE.ts`
- Backend: `system_stream.py` (SSE endpoint)

**P3 - Backend Endpoints (3-4 Std):**
- Business Factory API: 9 Endpoints (`/api/business/*`)
- Course Factory API: 8 Endpoints (`/api/courses/*`)
- Entfernt 17 TODOs aus Frontend Hooks

**P4 - UX Polish (1 Std):**
- Loading Skeletons (shadcn/ui Skeleton component)
- Error Boundaries mit Retry-Button
- Dashboard Telemetry Chart (echte Daten)

### Detaillierter Implementierungsplan (Plan Agent Output)

**‚úÖ Vollst√§ndiger Plan erstellt mit:**
- Phase 1: API Config Fix (30 min) - 5 Dateien, spezifische Zeilen
- Phase 2: Sidebar Restructuring (1 Std) - 3 Sektionen, Icon-Imports, Collapsible State
- Phase 3: WebSocket/SSE (2 Std) - Complete code examples f√ºr Hooks + Backend
- Phase 4: Backend APIs (3-4 Std) - Vollst√§ndige Schemas + 17 Endpoints
- Phase 5: UX Polish (1 Std) - Skeleton components + Error Boundaries

**Kritische Dateien identifiziert:**
- Frontend: 10 Dateien (5x API fix, 1x Sidebar, 4x Hooks/Components)
- Backend: 2 neue Dateien (business.py, courses.py)

**Gesch√§tzter Aufwand:** 8-10 Stunden total

### Design-Verbesserungen (shadcnblocks-admin Stil)

**Sidebar Modernisierung:**
- Collapsible sections mit Smooth Animations
- Icon-basierte Navigation (Lucide React Icons)
- Visual hierarchy: Farbakzente pro Sektion (Blue/Green/Purple)
- Active state indicators mit Border-Highlight

**Card & Layout Patterns:**
- Moderne Card-Layouts: `shadow-sm hover:shadow-md transition`
- Consistent spacing: `gap-4`, `p-6`, `space-y-4`
- Responsive Grid: `grid-cols-1 md:grid-cols-2 lg:grid-cols-3`

**Typography Hierarchy:**
- Page Headings: `text-2xl font-bold tracking-tight`
- Section Titles: `text-lg font-semibold`
- Descriptions: `text-muted-foreground text-sm`
- Stats/Metrics: `text-4xl font-bold`

**Component Improvements:**
- Loading States: Skeleton animations (pulse effect)
- Error States: Alert cards mit Icon + Retry button
- Empty States: Centered illustration + CTA
- Tables: Sortable headers, row hover effects, pagination

### Implementation Checklist

**Phase 1 - Quick Win (30 min):**
- [ ] Update 5 API files: dashboardApi, neurorailApi, coreOverviewApi, missionsApi, agentsApi
- [ ] Verify: `grep -r "NEXT_PUBLIC_BRAIN_API_URL" frontend/control_deck/lib/` returns 0 results
- [ ] Test: All API calls work with new env var

**Phase 2 - Sidebar (1 hour):**
- [ ] Import 10 new Lucide icons (Settings2, Activity, Wrench, etc.)
- [ ] Replace navMain array (lines 50-267) mit 3-section structure
- [ ] Add collapsible state management: `useState<Set<string>>`
- [ ] Update NavMain component f√ºr Collapsible support
- [ ] Test: All 37 pages accessible from new sidebar

**Phase 3 - WebSocket/SSE (2 hours):**
- [ ] Create `hooks/useMissionWebSocket.ts` (60 lines)
- [ ] Create `hooks/useHealthSSE.ts` (40 lines)
- [ ] Create `backend/api/routes/system_stream.py` (SSE endpoint)
- [ ] Integrate WebSocket in missions page
- [ ] Integrate SSE in health/telemetry pages
- [ ] Test: Real-time updates working, auto-reconnect on disconnect

**Phase 4 - Backend APIs (4-5 hours) - WITH POSTGRESQL:**
- [ ] **4.1 Database Models (1 hour):**
  - [ ] Create `backend/app/models/business.py` - SQLAlchemy models (BusinessProcess, ProcessStep, ProcessTrigger, ProcessExecution)
  - [ ] Create `backend/app/models/courses.py` - SQLAlchemy models (CourseTemplate, CourseModule, Lesson, Resource)
  - [ ] Create Alembic migration: `alembic revision --autogenerate -m "Add business and course factory tables"`
  - [ ] Apply migration: `alembic upgrade head`
  - [ ] Verify: Tables created in PostgreSQL
- [ ] **4.2 Backend Routes (2-3 hours):**
  - [ ] Create `backend/api/routes/business.py` (450+ lines, 9 endpoints with DB integration)
  - [ ] Create `backend/api/routes/courses.py` (350+ lines, 8 endpoints with DB integration)
  - [ ] Use async SQLAlchemy sessions for all DB operations
  - [ ] Add proper error handling and transactions
- [ ] **4.3 Frontend Integration (30 min):**
  - [ ] Update `hooks/useBusinessFactory.ts` - Remove 9 TODOs (lines 127-189)
  - [ ] Update `hooks/useCourseFactory.ts` - Remove 8 TODOs (lines 108-167)
  - [ ] Verify: 0 TODOs in factory hooks
- [ ] **4.4 Testing (30 min):**
  - [ ] Test: All CRUD operations for both factories
  - [ ] Test: Data persists after container restart
  - [ ] Test: Foreign key constraints work correctly

**Phase 5 - UX Polish (1 hour):**
- [ ] Install: `npx shadcn-ui@latest add skeleton`
- [ ] Create `components/skeletons/PageSkeleton.tsx` (3 variants)
- [ ] Apply skeletons to: Dashboard, Business, Courses, Missions, Telemetry
- [ ] Update `ErrorBoundary.tsx` - Add retry button
- [ ] Test: Loading states smooth, errors recoverable

### Testing Strategy

**Manual Testing Checklist:**
- [ ] Navigate through all 3 sidebar sections
- [ ] Verify all 37 pages load correctly
- [ ] Test Business Factory: Create ‚Üí Edit ‚Üí Delete ‚Üí Execute
- [ ] Test Course Factory: Create ‚Üí Edit ‚Üí Delete ‚Üí Publish
- [ ] Verify WebSocket connection in Network tab
- [ ] Verify SSE stream in Network tab
- [ ] Trigger loading states by throttling network
- [ ] Trigger error states by stopping backend

**Success Criteria:**
- ‚úÖ 0 API config inconsistencies
- ‚úÖ 0 TODOs in Business/Course Factory hooks
- ‚úÖ Real-time updates on 3+ pages
- ‚úÖ All 37 pages have loading skeletons
- ‚úÖ <500ms skeleton display time
- ‚úÖ Error states show retry button

### Rollback Plan

Falls Probleme auftreten:
1. **API Config:** Revert zu `NEXT_PUBLIC_BRAIN_API_URL`, add to `.env.local`
2. **Sidebar:** Revert `app-sidebar.tsx` zu Original
3. **WebSocket:** Remove hooks, restore `refetchInterval` polling
4. **Backend APIs:** Comment out new routes in `backend/api/routes/`
5. **UX:** Remove skeleton imports, restore "Loading..." text

### User-Entscheidungen ‚úÖ

1. **Backend Storage:** ‚úÖ **PostgreSQL** (User confirmed: "UCP Persistence bleibt PostgreSQL. Cache ist Cache ‚Äì Audit & Compliance brauchen ein persistentes Truth-System.")
   - Business/Course Factory: PostgreSQL Modelle + Alembic Migration
   - Audit/Compliance: Persistent storage (nicht in-memory)
   - Nur Cache f√ºr tempor√§re Daten

2. **Animation Library:** CSS-only (kein Framer Motion - einfacher)
3. **Deployment Strategie:** Nach jeder Phase deployen + testen
4. **Error Tracking:** Sp√§ter (nicht in diesem Sprint)

### N√§chste Schritte

1. ‚úÖ **User hat offene Fragen beantwortet** (PostgreSQL confirmed)
2. ‚úÖ **Plan finalisiert** - Alle 5 Phasen detailliert ausgearbeitet
3. **ExitPlanMode** ‚Üí Wechsel zu Execution Mode
4. **Implementation starten:** P1 ‚Üí P2 ‚Üí P3 ‚Üí P4 (in Reihenfolge)
   - P1: API Config Fix (30 min)
   - P2: WebSocket/SSE (2 Std)
   - P3: Sidebar Restructuring (1 Std)
   - P4: Backend APIs + PostgreSQL (4-5 Std)
   - P5: UX Polish (1 Std)
5. **Deploy + Test:** Nach jeder Phase deployen, `~/brain-test-v2.sh` ausf√ºhren
6. **Commit:** Nach erfolgreichem Test committen, Plan-Datei mitcommiten

**User Ready:** "Starte mit P1" ‚Üí Implementation beginnt! üöÄ

**Total Time:** 8.5-10 Stunden (erh√∂ht durch PostgreSQL Integration)

---

## üîç ANALYSE: WARUM OLLAMA PL√ñTZLICH PROBLEMATISCH (2026-01-16)

### Hintergrund
User meldet: "warum pl√∂tzlich Ollama nicht l√§uft. war sonst nie das Problem"
- Deployment schl√§gt fehl beim Pullen von Ollama (3.18GB Image)
- Vorher funktionierten Deployments mit Ollama problemlos
- User hat Ollama optional gemacht (Docker profiles), aber das war als "Fix" gedacht

### ROOT CAUSE IDENTIFIZIERT

**Commit c710473 (2026-01-16):** "fix(docker): Make ollama and qdrant optional with Docker profiles"

**Was passierte:**
1. ‚ùå **Problem gel√∂st:** Coolify Deployment-Timeout (3.18GB Download) durch `profiles: [local]`
2. ‚ùå **NEUES Problem geschaffen:** AXE Agent Initialisierung schl√§gt fehl

**Warum es jetzt bricht:**

```yaml
# docker-compose.yml (NACH c710473)
ollama:
  profiles:
    - local  # Nur mit: docker compose --profile local up
```

**Resultat:**
- Coolify: `docker compose up` ‚Üí Ollama wird NICHT gestartet (profile √ºbersprungen)
- Backend: Versucht zu connecten ‚Üí `OLLAMA_HOST=http://host.docker.internal:11434`
- Connection: FEHLSCHLAG ‚Üí AXE Agent Initialisierung fails
- System: Wird instabil

**VORHER (funktionierte):**
- Ollama war IMMER gestartet (kein profile)
- Coolify lud 3.18GB (dauerte lange, aber funktionierte)
- AXE Agent konnte immer LLM erreichen

---

### IST-ZUSTAND vs. ZIELE (Stand 2026-01-16)

#### Projekt-Ziele (CLAUDE.md v0.6.1)

| Ziel | Ist-Zustand | Alignment | Status |
|------|-------------|-----------|--------|
| **Backend Hardening** | NeuroRail Phase 1 complete (100%) | ‚úÖ 100% | On Track |
| **control_deck (PRIMARY)** | P1-P5 complete, 14+ pages, WebSocket/SSE | ‚úÖ 95% | Excellent |
| **axe_ui (SECONDARY)** | Skeleton ready, WebSocket pending | üü° 70% | In Progress |
| **brain_control_ui (FUTURE)** | Design pending | üü° 30% | Planned |
| **DSGVO/EU AI Act** | Policy Engine + NeuroRail audit | ‚úÖ 90% | Compliant |
| **Production Deployment** | Coolify working, Traefik optimized | ‚úÖ 85% | Functional |
| **Observability** | Prometheus + Audit Trail complete | ‚úÖ 95% | Excellent |
| **LLM Integration** | Ollama optional, external APIs supported | ‚ö†Ô∏è 65% | **ISSUE HERE** |

**Overall Alignment Score: 87%** - Strong alignment except for LLM integration issue

---

### √ÑNDERUNGEN SEIT SNAPSHOT

#### Timeline (Wichtigste Commits)

**Phase 1: P1-P5 Control Deck Improvements (2026-01-05 bis 2026-01-09)**
- ‚úÖ `f773376` - P1: API Config Consistency Fix (5 Dateien)
- ‚úÖ `5070b55` - P2: WebSocket/SSE Real-time Updates (3 neue Dateien)
- ‚úÖ `b6fd45e` - P3: Sidebar Restructuring (14 ‚Üí 3 Hauptbereiche)
- ‚úÖ `33ae1b6` - P4: Backend APIs + PostgreSQL (17 Endpoints, 9 Tabellen)
- ‚úÖ `00afc58` - P5: UX Polish (Skeletons + ErrorBoundary)

**Phase 2: Deployment Fixes (2026-01-16)**
- ‚úÖ `24f201e` - Fix: useHealthSSE dependency (removed use-sse)
- ‚úÖ `570cf16` - Fix: Build errors (duplicate imports, isLoading)
- ‚úÖ `bb0d7e0` - Update package-lock.json
- ‚ö†Ô∏è `c710473` - **KRITISCH:** Ollama optional (profiles) ‚Üê **HIER DAS PROBLEM**

**Statistik:**
- 7 Commits in 11 Tagen
- ~1500 Zeilen Code hinzugef√ºgt
- 17 REST Endpoints + 9 DB Tabellen
- Build-Zeit reduziert: 15+ min ‚Üí 2 min (durch Ollama-Skip)

---

### WARUM WAR OLLAMA VORHER KEIN PROBLEM?

#### Vorheriger Zustand (vor c710473)

**docker-compose.yml (ALT):**
```yaml
ollama:
  image: ollama/ollama:latest
  # KEIN profiles: [local]
  volumes:
    - brain_ollama_data:/root/.ollama
  restart: unless-stopped
```

**Deployment-Verhalten:**
1. Coolify startet Deployment
2. Docker pullt Ollama Image (3.18GB) - **dauert 10-15 Min**
3. Alle Container starten inkl. Ollama
4. Backend connect zu `http://ollama:11434` ‚úÖ FUNKTIONIERT
5. AXE Agent initialisiert erfolgreich ‚úÖ

**Warum "kein Problem"?**
- Ollama wurde IMMER gestartet
- Connection war garantiert (internes Docker-Netzwerk)
- Nur Nachteil: Lange Build-Zeit

---

### USER'S WICHTIGE AUSSAGE

> "AXE braucht das. Wir wollen Datenhoheit. Keine externen LLM √ºber API, nur durch einen Router und Filter."

**Bedeutung:**
- Ollama ist KEIN "nice-to-have" f√ºr Tests
- Ollama ist CORE REQUIREMENT f√ºr AXE Agent
- Externe LLMs (OpenAI, Claude API) sind NICHT akzeptabel f√ºr Production
- Ollama Modell wird sp√§ter ausgetauscht (z.B. gr√∂√üere Modelle)
- Router/Filter-Architektur bedeutet: LLM-Traffic muss durch BRAiN gehen

**Architektur-Implikation:**
```
User ‚Üí AXE UI ‚Üí Backend ‚Üí Ollama (intern) ‚Üí Antwort
                         ‚Üì
                  Policy Engine Filter
                  Foundation Layer Validation
```

**NICHT:**
```
User ‚Üí AXE UI ‚Üí Backend ‚Üí External LLM API ‚Üí Antwort
```

---

### BENCHMARK: SNAPSHOT als Referenzpunkt

#### Snapshot-Status (vermutlich ~2026-01-05)

**Was war IM Snapshot:**
- ‚úÖ Backend mit allen Core-Modulen
- ‚úÖ NeuroRail Phase 1 (m√∂glicherweise kurz vor Abschluss)
- ‚úÖ Ollama IMMER gestartet (kein profile)
- ‚úÖ AXE Agent funktional
- ‚úÖ Deployment auf Coolify (langsam aber stabil)

**Was kam NACH Snapshot:**
- P1-P5 Control Deck Improvements (gro√üe Erweiterung)
- Ollama optional (Versuch, Deployment zu beschleunigen)
- Build-Fehler-Fixes (TypeScript, Dependencies)

**Snapshot = Goldener Zustand:**
- Deployment: Langsam (15+ min) aber STABIL
- Alle Services: Funktional
- AXE Agent: Voll funktionsf√§hig

**Aktueller Zustand nach c710473:**
- Deployment: Schnell (2 min) aber BROKEN
- Ollama: √úbersprungen ‚Üí AXE Agent broken
- Trade-off: Speed vs. Functionality ‚ùå

---

### PROBLEM-ANALYSE: 3 Ebenen

#### Ebene 1: Docker Compose Konfiguration

**Aktuell:**
```yaml
ollama:
  profiles: [local]  # Nur mit --profile local
```

**Problem:** Coolify ruft standardm√§√üig `docker compose up` auf (OHNE --profile)

**L√∂sung-Optionen:**
1. Remove `profiles: [local]` ‚Üí Ollama immer gestartet (zur√ºck zu alt)
2. Coolify konfigurieren mit `--profile local` flag
3. Separates compose file: `docker-compose.ollama.yml`

---

#### Ebene 2: Backend LLM Client Konfiguration

**Aktuell (modules/llm_client.py):**
```python
# Default: Versucht Ollama
OLLAMA_HOST = "http://host.docker.internal:11434"  # oder "http://ollama:11434"
```

**Problem:** Kein Fallback wenn Ollama nicht verf√ºgbar

**L√∂sung-Optionen:**
1. Health-Check beim Startup ‚Üí Fail-Fast wenn Ollama fehlt
2. Graceful Fallback ‚Üí Warnung + externe LLM (gegen User-Anforderung!)
3. Mandatory Ollama ‚Üí Deployment blockieren wenn nicht verf√ºgbar

---

#### Ebene 3: Deployment-Strategie

**Aktuell:** "Schnelles Deployment ohne Ollama"
**User-Anforderung:** "AXE braucht Ollama f√ºr Datenhoheit"

**Konflikt:** Speed vs. Requirements

**L√∂sung-Optionen:**
1. **Pr√§feriert:** Ollama immer dabei (langsames Deployment akzeptieren)
2. Ollama einmalig pullen + cachen (Docker layer cache nutzen)
3. Separater Ollama-Service (einmalig deployen, mehrfach nutzen)

---

### EMPFOHLENE L√ñSUNG

#### Option 1: Revert to Always-On Ollama (EMPFOHLEN)

**Was tun:**
1. Remove `profiles: [local]` aus docker-compose.yml
2. Ollama ist wieder immer aktiv
3. Deployment dauert wieder 15+ min (akzeptabel f√ºr Stabilit√§t)

**Vorteile:**
- ‚úÖ Einfachste L√∂sung (1 Zeile √§ndern)
- ‚úÖ Sofort funktional
- ‚úÖ Entspricht User-Anforderung (AXE braucht Ollama)
- ‚úÖ Keine Architektur-√Ñnderungen

**Nachteile:**
- ‚è±Ô∏è Deployment dauert l√§nger (aber nur beim initialen Pull)
- üíæ Mehr Speicher (aber sowieso ben√∂tigt)

**Code-Change:**
```yaml
# docker-compose.yml
ollama:
  image: ollama/ollama:latest
  # profiles: [local]  ‚Üê REMOVE THIS LINE
  volumes:
    - brain_ollama_data:/root/.ollama
  restart: unless-stopped
  networks:
    - brain_internal
```

---

#### Option 2: Separate Ollama Service (LANGFRISTIG)

**Architektur:**
```
Coolify Deployment 1: BRAiN Core (Backend, Frontend, DB, Redis)
Coolify Deployment 2: Ollama Service (separater Container, einmalig)
```

**Vorteile:**
- ‚úÖ Ollama nur einmal pullen
- ‚úÖ Kann zwischen Deployments geteilt werden
- ‚úÖ Updates unabh√§ngig

**Nachteile:**
- üîß Komplexere Konfiguration
- üåê Netzwerk zwischen Deployments n√∂tig
- ‚è∞ Mehr Setup-Zeit

---

#### Option 3: Docker Layer Caching (COOLIFY-SPEZIFISCH)

**Idee:** Coolify k√∂nnte Ollama Image cachen

**Status:** Unklar ob Coolify das unterst√ºtzt (m√ºsste getestet werden)

---

### EMPFEHLUNG F√úR USER

**Kurzfristig (JETZT):**
1. ‚úÖ **Revert c710473:** Remove `profiles: [local]` aus docker-compose.yml
2. ‚úÖ **Commit + Push:** "revert: Make Ollama always-on for AXE agent requirement"
3. ‚úÖ **Redeploy:** Coolify deployment (dauert 15 min, aber STABIL)
4. ‚úÖ **Verify:** AXE Agent initialisiert erfolgreich

**Mittelfristig (SP√ÑTER):**
1. Untersuche Coolify Docker Layer Caching
2. Evaluiere separate Ollama Service Architektur
3. Dokumentiere: Ollama ist CORE REQUIREMENT (nicht optional)

**Langfristig (V2.0):**
1. Multi-LLM Router (Ollama primary, externe APIs als Fallback nur mit Policy-Approval)
2. LLM Model Management (Modelle on-demand laden/entladen)
3. LLM Request Filtering (Policy Engine Integration)

---

### LESSONS LEARNED

**Was wir gelernt haben:**
1. ‚ùå **Nicht optimieren ohne Requirements zu verstehen:** Ollama optional zu machen war ein Performance-Fix ohne zu beachten, dass es ein CORE REQUIREMENT ist
2. ‚úÖ **User-Anforderungen priorisieren:** "AXE braucht das" bedeutet NON-NEGOTIABLE
3. ‚úÖ **Deployment-Speed vs. Functionality:** Stabilit√§t > Speed
4. ‚úÖ **Dokumentation wichtig:** "Ollama optional f√ºr Tests" vs. "Ollama REQUIRED f√ºr Production" h√§tte klar sein m√ºssen

**F√ºr die Zukunft:**
- Vor Performance-Optimierungen: Requirements kl√§ren
- "Optional" Features dokumentieren mit USE CASE
- Deployment-Strategien mit User absprechen
- Core Dependencies nicht ohne Fallback entfernen

---

### N√ÑCHSTE SCHRITTE (NACH DEPLOYMENT)

1. ‚úÖ **Dokumentation Update:**
   - CLAUDE.md: Ollama als CORE REQUIREMENT markieren
   - docker-compose.yml: Kommentar warum Ollama NICHT optional ist
   - Deployment Docs: Ollama Download-Zeit erw√§hnen

2. ‚úÖ **LLM Configuration:**
   - Backend: Health-Check f√ºr Ollama beim Startup
   - Backend: Fail-Fast mit klarer Fehlermeldung wenn Ollama fehlt
   - Backend: Kein stiller Fallback zu externen APIs

3. ‚úÖ **Monitoring:**
   - Ollama Container Health in Telemetry
   - LLM Response-Time Tracking
   - Token Usage Monitoring

---

## ZUSAMMENFASSUNG F√úR USER (ALTE EMPFEHLUNG)

**Problem:**
Commit c710473 machte Ollama optional (Docker profiles) um Deployment zu beschleunigen (3.18GB Download vermeiden). ABER: AXE Agent braucht Ollama zwingend f√ºr Datenhoheit.

**Root Cause:**
- Ollama mit `profiles: [local]` ‚Üí Coolify √ºberspringt es (kein --profile flag)
- Backend versucht Connection ‚Üí Fehlschlag
- AXE Agent Initialisierung ‚Üí Broken

**Solution:**
Remove `profiles: [local]` ‚Üí Ollama ist wieder IMMER dabei (wie vorher).

**Trade-off:**
Deployment dauert wieder 15+ min (wegen 3.18GB Download), ABER System ist STABIL und funktional.

**Langfristig:**
Separate Ollama Service Architektur evaluieren (einmalig pullen, mehrfach nutzen).

---

## üöÄ NEUE EMPFEHLUNG: SEPARATE COOLIFY APPLICATIONS (2026-01-16 22:30)

### User-Entscheidung
User fragt: "was denkst du, wenn wir Ollama als separate Application in Coolify behandeln? Vielleicht auch f√ºr Redis, DB und Qdrant. W√ºrde das gehen?"

**Antwort: JA! Das ist die BESTE L√∂sung!** ‚úÖ

---

### Vorteile der Architektur

**1. Image Caching (HAUPTVORTEIL):**
- Ollama (3.18GB) wird **einmal** gepullt ‚Üí bleibt auf Server
- Zuk√ºnftige BRAiN Deploys: Nur ~200MB (Backend + Frontends)
- **Deployment-Zeit:** Von 15+ min auf **2-3 min** reduziert

**2. Persistenz & Stabilit√§t:**
- Services laufen unabh√§ngig ‚Üí BRAiN Redeploy st√∂rt sie nicht
- Ollama l√§uft **immer** ‚Üí AXE Agent hat garantierten Zugriff
- Keine versehentlichen Service-Restarts bei BRAiN-Updates

**3. Resource Management:**
- Coolify kann Memory/CPU pro Service begrenzen
- PostgreSQL: 2GB RAM dedicated
- Ollama: 4GB RAM f√ºr Modelle
- Redis: 512MB
- BRAiN Backend: Remaining resources

**4. Shared Infrastructure:**
- Andere Projekte k√∂nnen PostgreSQL/Redis nutzen
- Zentrale Database f√ºr mehrere Apps
- Cost-Efficient

**5. Bessere Wartung:**
- Separate Logs pro Service
- Unabh√§ngige Updates (Ollama Model wechseln ohne BRAiN Downtime)
- Einfacheres Troubleshooting

---

### Architektur-√úbersicht

**Coolify Project Structure:**
```
Coolify ‚Üí Root Team
‚îú‚îÄ‚îÄ BRAiN-Ollama (Docker Image - 3.18GB, einmalig)
‚îú‚îÄ‚îÄ BRAiN-PostgreSQL (Database - persistent)
‚îú‚îÄ‚îÄ BRAiN-Redis (Database - cache)
‚îú‚îÄ‚îÄ BRAiN-Qdrant (Docker Image - vector DB)
‚îî‚îÄ‚îÄ BRAiN-Main (Docker Compose - nur backend/frontends, ~200MB)
```

**Service Discovery (Docker Network):**
Alle Services im `coolify` Docker Network k√∂nnen sich √ºber Coolify-Namen erreichen:
- Ollama: `http://brain-ollama:11434`
- PostgreSQL: `brain-postgres:5432`
- Redis: `redis://brain-redis:6379`
- Qdrant: `http://brain-qdrant:6333`

---

### Vergleich: Vorher vs. Nachher

| Aspekt | Monolith (jetzt) | Separate Services |
|--------|------------------|-------------------|
| Deployment Zeit | 15+ min (3.5GB) | **2-3 min (200MB)** |
| Ollama Verf√ºgbarkeit | Nur wenn BRAiN l√§uft | **Immer verf√ºgbar** |
| Resource Control | Alle zusammen | **Pro Service** |
| Troubleshooting | Gemischte Logs | **Separate Logs** |
| Updates | Alles neu bauen | **Nur was n√∂tig ist** |
| Image Caching | Bei jedem Deploy | **Einmal pullen** |
| Skalierung | Nicht m√∂glich | **Horizontal skalierbar** |
| Multi-Project | Nur BRAiN | **Shared Infrastructure** |

---

### Implementierungs-Plan

#### Phase 1: Infrastruktur Services erstellen (Einmalig, 30 Min)

**1.1 BRAiN-PostgreSQL (Database Type)**

Coolify UI:
```
- Type: PostgreSQL
- Name: brain-postgres
- Version: 15 (oder latest mit pgvector)
- Database Name: brain
- Database User: brain
- Database Password: <generiert oder aus aktueller .env>
- Port: 5432 (internal only)
- Volume: brain_postgres_data
- Network: coolify (default)
```

**1.2 BRAiN-Redis (Database Type)**

Coolify UI:
```
- Type: Redis
- Name: brain-redis
- Version: 7-alpine
- Port: 6379 (internal only)
- Volume: brain_redis_data
- Network: coolify (default)
- Config: Default (no password for internal network)
```

**1.3 BRAiN-Qdrant (Docker Image)**

Coolify UI:
```
- Type: Docker Image
- Name: brain-qdrant
- Image: qdrant/qdrant:latest
- Port: 6333 (internal only)
- Volume: brain_qdrant_data:/qdrant/storage
- Network: coolify (default)
- Restart Policy: unless-stopped
```

**1.4 BRAiN-Ollama (Docker Image)**

Coolify UI:
```
- Type: Docker Image
- Name: brain-ollama
- Image: ollama/ollama:latest
- Port: 11434 (internal only)
- Volume: brain_ollama_data:/root/.ollama
- Network: coolify (default)
- Restart Policy: unless-stopped
- Memory Limit: 4GB (optional, f√ºr gro√üe Modelle)
```

**Deployment Order:**
1. PostgreSQL ‚Üí Wait for healthy
2. Redis ‚Üí Wait for healthy
3. Qdrant ‚Üí Wait for healthy
4. Ollama ‚Üí Wait for healthy (initialer Pull dauert 15 min, aber nur EINMAL!)

---

#### Phase 2: BRAiN-Main anpassen (10 Min)

**2.1 docker-compose.yml bereinigen**

**Entfernen (Services, die jetzt separat laufen):**
```yaml
# REMOVE THESE SERVICES:
postgres:
  # ... (wird zu brain-postgres)

redis:
  # ... (wird zu brain-redis)

qdrant:
  # ... (wird zu brain-qdrant)

ollama:
  # ... (wird zu brain-ollama)
```

**Behalten (nur diese Services):**
```yaml
version: '3.8'

services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    environment:
      # Updated to use separate services (see 2.2)
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - OLLAMA_HOST=${OLLAMA_HOST}
      - QDRANT_HOST=${QDRANT_HOST}
    ports:
      - "8000:8000"
    networks:
      - coolify  # Important: Same network as separate services
    restart: unless-stopped

  control_deck:
    build:
      context: ./frontend/control_deck
      dockerfile: Dockerfile
    environment:
      - NEXT_PUBLIC_BRAIN_API_BASE=https://api.dev.brain.falklabs.de
    ports:
      - "3000:3000"
    networks:
      - coolify
    restart: unless-stopped

  axe_ui:
    build:
      context: ./frontend/axe_ui
      dockerfile: Dockerfile
    environment:
      - NEXT_PUBLIC_BRAIN_API_BASE=https://api.dev.brain.falklabs.de
    ports:
      - "3001:3000"
    networks:
      - coolify
    restart: unless-stopped

networks:
  coolify:
    external: true  # Use Coolify's network
```

---

**2.2 Environment Variables aktualisieren**

**backend/.env (oder Coolify ENV Settings):**

```bash
# ALTE Werte (interne Service-Namen im Compose):
DATABASE_URL=postgresql://brain:password@postgres:5432/brain
REDIS_URL=redis://redis:6379/0
OLLAMA_HOST=http://ollama:11434
QDRANT_HOST=http://qdrant

# NEUE Werte (Coolify Service-Namen):
DATABASE_URL=postgresql://brain:password@brain-postgres:5432/brain
REDIS_URL=redis://brain-redis:6379/0
OLLAMA_HOST=http://brain-ollama:11434
QDRANT_HOST=http://brain-qdrant
QDRANT_PORT=6333
```

**Wichtig:** Coolify Service-Namen sind die Container-Namen im `coolify` Docker Network!

---

**2.3 Coolify BRAiN-Main Settings aktualisieren**

Coolify UI ‚Üí BRAiN-Main Project ‚Üí Settings:
```
Network: coolify (ensure it's the SAME network as separate services)
```

---

#### Phase 3: Deployment & Testing (15 Min)

**3.1 Erste Deployment der Infrastruktur (einmalig)**

SSH auf Server:
```bash
ssh root@brain.falklabs.de

# 1. Verify separate services are running
docker ps | grep -E "(brain-postgres|brain-redis|brain-qdrant|brain-ollama)"

# Expected: 4 containers UP

# 2. Test connectivity from coolify network
docker run --rm --network coolify alpine:latest ping -c 2 brain-postgres
docker run --rm --network coolify alpine:latest ping -c 2 brain-redis
docker run --rm --network coolify alpine:latest ping -c 2 brain-qdrant
docker run --rm --network coolify alpine:latest ping -c 2 brain-ollama

# Expected: All return "2 packets transmitted, 2 received"
```

**3.2 BRAiN-Main Deployment**

Coolify UI:
1. Update docker-compose.yml (remove postgres/redis/qdrant/ollama services)
2. Update backend/.env (new service names)
3. Commit & Push to GitHub
4. Coolify ‚Üí BRAiN-Main ‚Üí "Force Deploy"
5. **Erwartung:** Build dauert nur 2-3 Min (kein 3.18GB Download!)

**3.3 Verification**

SSH auf Server:
```bash
# 1. Check container status
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Size}}"

# Expected:
# brain-ollama          Up 20 minutes    3.18GB
# brain-postgres        Up 20 minutes    500MB
# brain-redis           Up 20 minutes    50MB
# brain-qdrant          Up 20 minutes    200MB
# backend-mw0...        Up 3 minutes     150MB  ‚Üê SCHNELL NEU GEBAUT!
# control_deck-mw0...   Up 3 minutes     80MB
# axe_ui-mw0...         Up 3 minutes     60MB

# 2. Test backend connection to services
docker exec $(docker ps -q --filter "name=backend-mw0") sh -c "
  curl -s http://brain-ollama:11434/api/tags | jq . &&
  curl -s http://brain-qdrant:6333/health | jq .
"

# Expected: JSON responses from Ollama and Qdrant

# 3. Run test suite
~/brain-test-v2.sh

# Expected: 16/16 tests passed (100%)
```

---

#### Phase 4: Update Dokumentation (10 Min)

**4.1 CLAUDE.md aktualisieren**

Add new section:
```markdown
### Separate Infrastructure Services (Coolify Deployment)

BRAiN uses separate Coolify applications for infrastructure services to optimize deployment speed and resource management:

**Architecture:**
- **BRAiN-Ollama** (3.18GB, einmalig) - LLM inference service
- **BRAiN-PostgreSQL** - Persistent database
- **BRAiN-Redis** - Cache and mission queue
- **BRAiN-Qdrant** - Vector database
- **BRAiN-Main** (~200MB) - Backend + Frontends

**Vorteile:**
- Deployment-Zeit: 15+ min ‚Üí 2-3 min
- Ollama immer verf√ºgbar (unabh√§ngig von BRAiN Updates)
- Shared infrastructure f√ºr mehrere Projekte m√∂glich

**Service Discovery:**
Services kommunizieren √ºber Coolify Docker Network:
- `brain-ollama:11434`
- `brain-postgres:5432`
- `brain-redis:6379`
- `brain-qdrant:6333`
```

**4.2 docker-compose.yml Kommentar hinzuf√ºgen**

```yaml
# ===================================================================
# BRAiN Main Services (Backend + Frontends only)
# ===================================================================
# NOTE: Infrastructure services (PostgreSQL, Redis, Ollama, Qdrant)
# are deployed as SEPARATE Coolify applications for:
# - Faster deployments (no 3.18GB Ollama re-download)
# - Independent lifecycle (updates don't restart infrastructure)
# - Shared infrastructure across projects
#
# Service Names in Coolify Network:
# - brain-postgres:5432
# - brain-redis:6379
# - brain-ollama:11434
# - brain-qdrant:6333
# ===================================================================
```

**4.3 README.md Deployment Section aktualisieren**

```markdown
## Deployment Architecture

BRAiN v2 verwendet eine **Separate Infrastructure Services** Architektur:

### Infrastructure Services (Einmalig deployen)
1. BRAiN-PostgreSQL (Database)
2. BRAiN-Redis (Cache)
3. BRAiN-Ollama (LLM, 3.18GB)
4. BRAiN-Qdrant (Vector DB)

### Main Application (H√§ufig deployen)
- BRAiN Backend (FastAPI)
- Control Deck (Next.js)
- AXE UI (Next.js)

**Deployment-Zeit:**
- Infrastruktur: ~15 Min (nur einmalig)
- Main App: **2-3 Min** (jedes Update)
```

---

### Rollback Plan

Falls Probleme auftreten:

**Option A: Zur√ºck zum Monolith**
```bash
# 1. Revert docker-compose.yml (re-add postgres/redis/ollama/qdrant)
git revert <commit-hash>

# 2. Revert backend/.env (alte Service-Namen)
DATABASE_URL=postgresql://brain:password@postgres:5432/brain

# 3. Redeploy BRAiN-Main
Coolify ‚Üí Force Deploy
```

**Option B: Infrastruktur Services behalten, aber testen**
```bash
# Nur BRAiN-Main neu deployen mit alten ENV
# Services bleiben separat
```

---

### Troubleshooting

**Problem: BRAiN-Main kann Services nicht erreichen**

```bash
# 1. Verify network
docker network inspect coolify | grep -A 10 '"Containers"'

# Expected: Alle services (brain-postgres, brain-redis, brain-ollama, brain-qdrant, backend, control_deck, axe_ui)

# 2. Test DNS resolution
docker exec $(docker ps -q --filter "name=backend-mw0") nslookup brain-ollama

# Expected: IP address returned

# 3. Test connectivity
docker exec $(docker ps -q --filter "name=backend-mw0") curl -s http://brain-ollama:11434/api/tags

# Expected: JSON response
```

**Fix:** Ensure all services are on `coolify` network:
```bash
docker network connect coolify brain-ollama
docker network connect coolify brain-postgres
docker network connect coolify brain-redis
docker network connect coolify brain-qdrant
```

---

**Problem: Ollama Model nicht geladen**

```bash
# 1. Check Ollama logs
docker logs brain-ollama --tail 50

# 2. Pull model manually
docker exec brain-ollama ollama pull llama3.2:latest

# 3. Verify model loaded
docker exec brain-ollama ollama list
```

---

**Problem: PostgreSQL Connection refused**

```bash
# 1. Check PostgreSQL logs
docker logs brain-postgres --tail 50

# 2. Verify database exists
docker exec brain-postgres psql -U brain -c "\l"

# 3. Test connection
docker exec brain-postgres psql -U brain -d brain -c "SELECT 1;"
```

---

### Execution Checklist

**Phase 1: Infrastruktur Services (Einmalig)**
- [ ] Coolify: Create BRAiN-PostgreSQL application
- [ ] Coolify: Create BRAiN-Redis application
- [ ] Coolify: Create BRAiN-Qdrant application
- [ ] Coolify: Create BRAiN-Ollama application
- [ ] Wait for all services healthy (~15 min for Ollama initial pull)
- [ ] Verify: All 4 services UP in `docker ps`

**Phase 2: BRAiN-Main Anpassungen**
- [ ] docker-compose.yml: Remove postgres/redis/qdrant/ollama services
- [ ] docker-compose.yml: Add network: coolify (external: true)
- [ ] backend/.env: Update service names (brain-postgres, brain-redis, etc.)
- [ ] Commit changes to GitHub (branch: `claude/separate-infra-services`)

**Phase 3: Deployment & Testing**
- [ ] Test connectivity: ping brain-postgres, brain-redis, brain-qdrant, brain-ollama
- [ ] Coolify: Force Deploy BRAiN-Main
- [ ] Deployment duration: Should be ~2-3 min (NOT 15+ min!)
- [ ] Verify: Backend logs show successful connections
- [ ] Run: `~/brain-test-v2.sh` ‚Üí 16/16 tests passed

**Phase 4: Dokumentation**
- [ ] CLAUDE.md: Add "Separate Infrastructure Services" section
- [ ] docker-compose.yml: Add comment explaining architecture
- [ ] README.md: Update Deployment section
- [ ] Commit documentation updates

**Phase 5: Cleanup**
- [ ] Create PR: `claude/separate-infra-services` ‚Üí `v2`
- [ ] Merge PR after successful test
- [ ] Delete old unused volumes (optional)

---

### Erfolgs-Kriterien

‚úÖ **Phase 1 Erfolg:**
- 4 separate Coolify applications laufen
- Alle Services erreichbar im `coolify` network

‚úÖ **Phase 2 Erfolg:**
- docker-compose.yml nur noch 3 services (backend, control_deck, axe_ui)
- ENV variables aktualisiert mit neuen Service-Namen

‚úÖ **Phase 3 Erfolg:**
- BRAiN-Main Deployment dauert **< 5 Min** (vorher 15+ min)
- Alle Tests bestehen: `~/brain-test-v2.sh` ‚Üí 16/16
- Backend connected zu allen Services

‚úÖ **Phase 4 Erfolg:**
- Dokumentation vollst√§ndig aktualisiert
- Andere Entwickler k√∂nnen Setup nachvollziehen

---

### Zeitsch√§tzung

| Phase | Dauer | Details |
|-------|-------|---------|
| Phase 1 | 30 Min | Infrastruktur Setup (15 min Ollama Pull einmalig) |
| Phase 2 | 10 Min | docker-compose.yml + ENV updates |
| Phase 3 | 15 Min | Deployment + Testing |
| Phase 4 | 10 Min | Dokumentation |
| **Total** | **65 Min** | **Einmalig**, dann immer schnell! |

**Zuk√ºnftige Deployments:** Nur 2-3 Min (Phase 3 only)

---

### USER READY TO START? üöÄ

**N√§chster Schritt:** ExitPlanMode ‚Üí Execution beginnen

**Start mit:** Phase 1 - Coolify UI Infrastructure Services erstellen

---

## üéØ PHASE 1 - KONKRETE SCHRITTE (Basierend auf Coolify Screenshots)

### User Entscheidungen ‚úÖ
- **JETZT:** Kern-Services (PostgreSQL, Redis, Ollama, Qdrant)
- **SP√ÑTER:** OpenWebUI (separate Ressource reaktivieren)
- **SP√ÑTER:** ClickHouse (Analytics), Prometheus/Grafana (Monitoring)
- **Strategie:** Erst Kern, dann erweitern

---

### Schritt 1.1: PostgreSQL erstellen (5 Min)

**Coolify UI:**
1. Gehe zu: **BRAiN Project** ‚Üí **+ Add Resource**
2. W√§hle: **Databases** Tab
3. Klicke: **PostgreSQL**
4. Konfiguration:
   ```
   Name: brain-postgres
   Version: 16 (latest mit pgvector support)
   Database Name: brain
   Database User: brain
   Database Password: <aus aktueller .env kopieren oder neu generieren>
   Port: 5432 (internal only, nicht publishen)
   Volume: Automatic (brain_postgres_data)
   Network: coolify (default)
   Server: localhost
   ```
5. Klicke: **Save**
6. Warte auf: Status "Running" (gr√ºner Punkt)

**Verification:**
```bash
ssh root@brain.falklabs.de
docker ps | grep postgres
# Expected: brain-postgres-... container Running
```

---

### Schritt 1.2: Redis erstellen (3 Min)

**Coolify UI:**
1. **BRAiN Project** ‚Üí **+ Add Resource**
2. **Databases** Tab ‚Üí **Redis**
3. Konfiguration:
   ```
   Name: brain-redis
   Version: 7-alpine (latest)
   Password: <leer lassen f√ºr internal network>
   Port: 6379 (internal only)
   Volume: Automatic (brain_redis_data)
   Network: coolify (default)
   Server: localhost
   ```
4. **Save**
5. Warte auf: Status "Running"

**Verification:**
```bash
docker ps | grep redis
# Expected: brain-redis-... container Running
```

---

### Schritt 1.3: Qdrant erstellen (5 Min)

**Coolify UI:**
1. **BRAiN Project** ‚Üí **+ Add Resource**
2. **Applications** Tab ‚Üí **Docker Image**
3. Konfiguration:
   ```
   Image Name: qdrant/qdrant:latest
   Tag: latest (optional)
   Name: brain-qdrant
   Port Mappings: 6333 (internal only)
   Volume: /qdrant/storage ‚Üí Persistent Volume (brain_qdrant_data)
   Network: coolify (default)
   Server: localhost
   Environment Variables: (keine n√∂tig)
   Restart Policy: unless-stopped
   ```
4. **Save**
5. Deploy startet automatisch
6. Warte auf: Status "Running"

**Verification:**
```bash
docker ps | grep qdrant
curl http://localhost:6333/health
# Expected: {"status":"ok"}
```

---

### Schritt 1.4: Ollama erstellen (15-20 Min - Image Pull)

**Coolify UI:**
1. **BRAiN Project** ‚Üí **+ Add Resource**
2. **Applications** Tab ‚Üí **Docker Image**
3. Konfiguration:
   ```
   Image Name: ollama/ollama:latest
   Tag: latest
   Name: brain-ollama
   Port Mappings: 11434 (internal only)
   Volume: /root/.ollama ‚Üí Persistent Volume (brain_ollama_data)
   Network: coolify (default)
   Server: localhost
   Environment Variables: (keine n√∂tig)
   Resource Limits:
     Memory: 4GB (optional, f√ºr gro√üe Modelle)
   Restart Policy: unless-stopped
   ```
4. **Save**
5. Deploy startet ‚Üí **Image Pull dauert 15-20 Min (3.18GB!)**
6. Warte auf: Status "Running"

**Verification:**
```bash
docker ps | grep ollama
docker logs <ollama-container-name> --tail 20

# Test Ollama API
curl http://localhost:11434/api/tags
# Expected: {"models":[]} (leer, Modelle sp√§ter pullen)
```

**Modell laden (optional, nach Deploy):**
```bash
docker exec <ollama-container-name> ollama pull llama3.2:latest
# Dauert nochmal 10-15 Min (2GB Model)
```

---

### Schritt 1.5: Alle Services verifizieren (5 Min)

**SSH auf Server:**
```bash
ssh root@brain.falklabs.de

# 1. Check: Alle 4 Container laufen
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}" | grep -E "(brain-postgres|brain-redis|brain-qdrant|brain-ollama)"

# Expected Output (√§hnlich):
# brain-postgres-xyz    Up 10 minutes   5432/tcp
# brain-redis-xyz       Up 10 minutes   6379/tcp
# brain-qdrant-xyz      Up 8 minutes    6333/tcp
# brain-ollama-xyz      Up 5 minutes    11434/tcp

# 2. Check: Alle im coolify network
docker network inspect coolify | grep -E "(brain-postgres|brain-redis|brain-qdrant|brain-ollama)"

# Expected: 4 Container im coolify network

# 3. Test Connectivity (vom coolify network aus)
docker run --rm --network coolify alpine:latest sh -c "
  ping -c 2 brain-postgres && \
  ping -c 2 brain-redis && \
  ping -c 2 brain-qdrant && \
  ping -c 2 brain-ollama
"

# Expected: All ping successful (2 packets transmitted, 2 received)
```

---

### Erfolgs-Kriterien Phase 1 ‚úÖ

- [ ] PostgreSQL: Status "Running", Port 5432, Volume angelegt
- [ ] Redis: Status "Running", Port 6379, Volume angelegt
- [ ] Qdrant: Status "Running", Port 6333, Volume angelegt, `/health` ‚Üí `{"status":"ok"}`
- [ ] Ollama: Status "Running", Port 11434, Volume angelegt, `/api/tags` ‚Üí JSON response
- [ ] Alle 4 Container im `coolify` Docker Network
- [ ] Ping successful zwischen allen Services

**Zeitaufwand Phase 1:** 30-35 Min (Ollama Pull = l√§ngster Teil)

---

### Schritt 1.6 (OPTIONAL): OpenWebUI reaktivieren (SP√ÑTER)

**User Statement:** "OPENWEBUI m√ºssen wir auch wieder aktivieren und verf√ºgbar machen. Auch als separate Ressource"

**Plan f√ºr sp√§ter:**

**Coolify UI:**
1. **BRAiN Project** ‚Üí **+ Add Resource**
2. **Applications** Tab ‚Üí **Docker Image**
3. Konfiguration:
   ```
   Image Name: ghcr.io/open-webui/open-webui:main
   Tag: main (oder latest)
   Name: brain-openwebui
   Port Mappings:
     - 3000 ‚Üí Public (√ºber Traefik)
     - Domain: openwebui.dev.brain.falklabs.de
   Volume:
     - /app/backend/data ‚Üí Persistent Volume (brain_openwebui_data)
   Network: coolify
   Environment Variables:
     - OLLAMA_BASE_URL=http://brain-ollama:11434
     - ENABLE_SIGNUP=false (oder true f√ºr Registrierung)
   Restart Policy: unless-stopped
   ```
4. **Save** ‚Üí Deploy

**Features:**
- Multi-LLM Interface (Ollama + externe APIs)
- Custom Branding m√∂glich
- Separate User-Base (third-party users, paid service)

**Zeitpunkt:** Nach Phase 3 (wenn BRAiN-Main deployed ist)

---

### N√§chste Schritte nach Phase 1:

**Phase 2:** BRAiN-Main anpassen (docker-compose.yml, ENV variables)
**Phase 3:** Deployment & Testing
**Phase 4:** Dokumentation

**Sobald Phase 1 abgeschlossen:** User meldet sich ‚Üí Ich starte mit Phase 2

---

