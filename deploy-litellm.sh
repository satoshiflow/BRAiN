#!/bin/bash
# Deploy LiteLLM Multi-Provider Setup
# Super einfach - macht alles automatisch!

set -e

echo "üöÄ LiteLLM Multi-Provider Setup"
echo "================================"
echo ""

cd /opt/brain

# 1. Neueste √Ñnderungen holen
echo "üì• 1. Hole neueste √Ñnderungen..."
git pull origin claude/migrate-v2-launch-01UQ1FuiVg8Rv6UQwwDar1g5
echo "‚úì Repository aktualisiert"
echo ""

# 2. API Keys konfigurieren (optional)
echo "üîê 2. API Keys konfigurieren..."
if ! grep -q "OPENAI_API_KEY" .env 2>/dev/null; then
    echo "# LiteLLM API Keys (optional - leer lassen f√ºr nur Ollama)" >> .env
    echo "OPENAI_API_KEY=" >> .env
    echo "ANTHROPIC_API_KEY=" >> .env
    echo "‚úì .env vorbereitet"
else
    echo "‚úì API Keys bereits in .env"
fi
echo ""
echo "üí° Tipp: Editiere /opt/brain/.env um API Keys hinzuzuf√ºgen:"
echo "   nano /opt/brain/.env"
echo "   F√ºge hinzu: OPENAI_API_KEY=sk-..."
echo "               ANTHROPIC_API_KEY=sk-ant-..."
echo ""

# 3. Mehr Ollama Modelle herunterladen (optional)
echo "ü§ñ 3. Ollama Modelle..."
CURRENT_MODELS=$(docker exec brain-ollama ollama list | tail -n +2 | wc -l)
echo "   Aktuelle Modelle: $CURRENT_MODELS"

if [ "$CURRENT_MODELS" -lt 2 ]; then
    echo ""
    echo "M√∂chtest du mehr Modelle herunterladen? (empfohlen)"
    echo "   1) llama3.2:3b (2GB) - Schnell, bereits heruntergeladen"
    echo "   2) qwen2.5:7b (4.7GB) - Sehr gut f√ºr Code"
    echo "   3) mistral:7b (4.1GB) - Allgemeine Aufgaben"
    echo ""
    read -p "Modell herunterladen? [2/3/n]: " choice

    case $choice in
        2)
            echo "Lade qwen2.5:7b herunter..."
            docker exec brain-ollama ollama pull qwen2.5:7b
            ;;
        3)
            echo "Lade mistral:7b herunter..."
            docker exec brain-ollama ollama pull mistral:7b
            ;;
        *)
            echo "√úberspringe Modell-Download"
            ;;
    esac
fi
echo ""

# 4. LiteLLM starten
echo "üîÑ 4. Starte LiteLLM..."
docker compose up -d litellm
echo "‚úì LiteLLM gestartet"
echo ""

# 5. Warte bis LiteLLM bereit ist
echo "‚è≥ 5. Warte auf LiteLLM..."
sleep 5

for i in {1..10}; do
    if curl -s http://localhost:4000/health > /dev/null 2>&1; then
        echo "‚úì LiteLLM ist bereit!"
        break
    fi
    if [ $i -eq 10 ]; then
        echo "‚ö†Ô∏è  LiteLLM braucht noch etwas Zeit..."
        echo "   Pr√ºfe Status mit: docker compose logs litellm"
    fi
    sleep 2
done
echo ""

# 6. Status anzeigen
echo "üìä 6. Status-√úbersicht:"
echo "-------------------"
docker compose ps | grep -E "NAME|brain-ollama|brain-litellm|brain-openwebui"
echo ""

# 7. Verf√ºgbare Modelle
echo "ü§ñ 7. Verf√ºgbare Modelle:"
echo "--------------------"
docker exec brain-ollama ollama list
echo ""

# 8. LiteLLM testen
echo "üß™ 8. LiteLLM Test:"
echo "---------------"
echo "Health Check:"
curl -s http://localhost:4000/health | head -5
echo ""
echo ""

# 9. Konfiguration anzeigen
echo "‚úÖ Setup komplett!"
echo "=================="
echo ""
echo "üìã N√§chste Schritte:"
echo ""
echo "1Ô∏è‚É£  OpenWebUI mit LiteLLM verbinden:"
echo "   ‚Üí √ñffne: https://chat.falklabs.de"
echo "   ‚Üí Settings ‚Üí Connections"
echo "   ‚Üí OpenAI API:"
echo "     - Base URL: http://litellm:4000"
echo "     - API Key: sk-brain-secret-key-change-this"
echo ""
echo "2Ô∏è‚É£  Verf√ºgbare Modelle in LiteLLM:"
echo "   ‚Üí llama3.2-3b (Ollama - lokal)"
echo "   ‚Üí qwen2.5-7b (Ollama - lokal, falls installiert)"
echo "   ‚Üí mistral-7b (Ollama - lokal, falls installiert)"
echo "   ‚Üí gpt-4o (OpenAI - falls API Key gesetzt)"
echo "   ‚Üí claude-3-5-sonnet (Anthropic - falls API Key gesetzt)"
echo ""
echo "3Ô∏è‚É£  Optional: API Keys hinzuf√ºgen:"
echo "   nano /opt/brain/.env"
echo "   # Dann Container neu starten:"
echo "   docker compose restart litellm"
echo ""
echo "üìñ Logs anschauen:"
echo "   docker compose logs -f litellm"
echo ""
echo "üåê URLs:"
echo "   OpenWebUI: https://chat.falklabs.de"
echo "   Control Deck: https://brain.falklabs.de"
echo "   LiteLLM API: http://localhost:4000"
echo ""
