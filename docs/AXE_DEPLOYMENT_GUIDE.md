# AXE Chat Deployment Guide

**Datum:** 2026-02-18
**Status:** ‚úÖ PRODUCTION READY
**Version:** BRAiN v0.3.0

---

## üìã √úberblick

Dieses Dokument beschreibt die erfolgreiche Deployment und Konfiguration des AXE Chat Systems innerhalb der BRAiN-Infrastruktur.

### Komponenten

1. **AXE UI** - Next.js Frontend f√ºr Chat-Interface
2. **AXE Fusion** - Backend-Modul f√ºr Chat-API
3. **AXEllm** - OpenAI-kompatible API f√ºr Ollama
4. **Ollama** - LLM-Service (qwen2.5:0.5b)

---

## üèóÔ∏è Architektur

```
User Browser
    ‚Üì
AXE UI (Next.js)
    ‚Üì https://axe.brain.falklabs.de
BRAiN Backend API
    ‚Üì /api/axe/chat
AXE Fusion Service
    ‚Üì Internal HTTP
AXEllm (OpenAI-Compatible Wrapper)
    ‚Üì http://xkg0gc00sgcg0sc0g8wowskw-180855623729:11434
Ollama (qwen2.5:0.5b)
    ‚Üì
LLM Response
```

### Shared Ollama Architecture

**Ein Ollama f√ºr alle Services:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Ollama Container                ‚îÇ
‚îÇ  xkg0gc00sgcg0sc0g8wowskw-180855623729 ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  Network Aliases:                       ‚îÇ
‚îÇ  - ollama                               ‚îÇ
‚îÇ  - xkg0gc00sgcg0sc0g8wowskw-180855... ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚ñº                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Backend  ‚îÇ    ‚îÇ  AXEllm  ‚îÇ
‚îÇ API      ‚îÇ    ‚îÇ  Service ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Vorteile:**
- ‚úÖ Nur ein Ollama-Prozess (Ressourcen-effizient)
- ‚úÖ Modell nur einmal im RAM (397 MB statt 794 MB)
- ‚úÖ Beide Services nutzen gleiche Modelle
- ‚úÖ Einfachere Wartung

---

## üöÄ Deployment Steps

### 1. AXE Stack Service (Coolify)

**Service Config (`docker-compose.axe-stack.yml`):**

```yaml
version: '3.8'
services:
  axellm:
    image: 'ghcr.io/satoshiflow/brain/axellm:latest'
    container_name: axellm
    networks:
      - coolify
    environment:
      - 'OLLAMA_BASE_URL=http://xkg0gc00sgcg0sc0g8wowskw-180855623729:11434'
      - 'DEFAULT_MODEL=qwen2.5:0.5b'
      - REQUEST_TIMEOUT_SECONDS=60
      - LOG_LEVEL=info
    restart: unless-stopped
networks:
  coolify:
    external: true
```

**Wichtig:**
- ‚ö†Ô∏è Coolify speichert Docker Compose in seiner **Datenbank**, nicht aus Git
- ‚úÖ √Ñnderungen m√ºssen √ºber Coolify API oder UI gemacht werden
- ‚úÖ `OLLAMA_BASE_URL` muss den **Container-Namen** verwenden, nicht Service-Namen

### 2. Ollama Network Alias Setup

**Problem:** Backend sucht `http://ollama:11434` aber Container hei√üt `xkg0gc00sgcg0sc0g8wowskw-180855623729`

**L√∂sung - Netzwerk-Alias hinzuf√ºgen:**

```bash
# Container von Netzwerk trennen
docker network disconnect coolify xkg0gc00sgcg0sc0g8wowskw-180855623729

# Mit Alias wieder verbinden
docker network connect \
  --alias ollama \
  --alias xkg0gc00sgcg0sc0g8wowskw-180855623729 \
  coolify \
  xkg0gc00sgcg0sc0g8wowskw-180855623729

# Backend neu starten
docker restart vosss8wcg8cs80kcss8cgccc-193229452316
```

**Verifizierung:**

```bash
docker inspect xkg0gc00sgcg0sc0g8wowskw-180855623729 \
  | jq '.[0].NetworkSettings.Networks.coolify.Aliases'

# Expected Output:
# [
#   "ollama",
#   "xkg0gc00sgcg0sc0g8wowskw-180855623729"
# ]
```

### 3. Model Setup

**Model Pull:**

```bash
docker exec xkg0gc00sgcg0sc0g8wowskw-180855623729 \
  ollama pull qwen2.5:0.5b

# Verify
docker exec xkg0gc00sgcg0sc0g8wowskw-180855623729 \
  ollama list
```

**Model Info:**
- Name: `qwen2.5:0.5b`
- Size: 397 MB
- Language: Multilingual (EN, DE, etc.)

---

## üß™ Testing & Verification

### Quick Test Script

**File:** `test_axe_quick.sh`

```bash
#!/bin/bash
echo "=== Quick AXE Test ==="
echo ""
echo "1. AXE Health:"
curl -s https://api.brain.falklabs.de/api/axe/health | jq .
echo ""
echo "2. Chat Test:"
curl -s -X POST https://api.brain.falklabs.de/api/axe/chat \
  -H "Content-Type: application/json" \
  -d '{"model":"qwen2.5:0.5b","messages":[{"role":"user","content":"Say hello"}]}' \
  | jq '.text' 2>/dev/null || echo "ERROR"
```

### Full Stack Test

```bash
# Backend Health
curl -s https://api.brain.falklabs.de/health | jq .

# Expected:
# {
#   "status": "healthy",
#   "ollama_host": "http://ollama:11434",
#   "ollama_reachable": true,
#   "models_available": 1,
#   "models": ["qwen2.5:0.5b"]
# }

# AXE Health
curl -s https://api.brain.falklabs.de/api/axe/health | jq .

# Expected:
# {
#   "status": "healthy",
#   "axellm": "reachable",
#   "error": null
# }

# Chat Test
curl -s -X POST https://api.brain.falklabs.de/api/axe/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:0.5b",
    "messages": [
      {"role": "user", "content": "Was ist 2+2?"}
    ]
  }' | jq .

# Expected:
# {
#   "text": "Die Antwort ist 4...",
#   "raw": { ... }
# }
```

---

## üîç Troubleshooting

### Problem: "AXEllm Service nicht verf√ºgbar (503)"

**Ursache:** AXEllm kann Ollama nicht erreichen

**Diagnose:**

```bash
# Check AXEllm logs
docker logs --tail 50 axellm-mg400ss0o80gcs0owo0ckskc

# Check environment variable
docker exec axellm-mg400ss0o80gcs0owo0ckskc env | grep OLLAMA_BASE_URL

# Check if both containers are in same network
docker inspect axellm-mg400ss0o80gcs0owo0ckskc | jq '.[0].NetworkSettings.Networks | keys'
docker inspect xkg0gc00sgcg0sc0g8wowskw-180855623729 | jq '.[0].NetworkSettings.Networks | keys'
```

**Fix:** Update `OLLAMA_BASE_URL` to use correct container name

### Problem: "Cannot connect to Ollama service"

**Ursache:** Backend findet Ollama nicht unter `http://ollama:11434`

**Diagnose:**

```bash
# Check backend logs
docker logs --tail 50 vosss8wcg8cs80kcss8cgccc-193229452316

# Check Ollama aliases
docker inspect xkg0gc00sgcg0sc0g8wowskw-180855623729 \
  | jq '.[0].NetworkSettings.Networks.coolify.Aliases'
```

**Fix:** Add network alias (siehe Deployment Steps #2)

### Problem: Chat returns null or error

**Checklist:**

1. ‚úÖ Model name korrekt? (`qwen2.5:0.5b` nicht `qwen:0.5b`)
2. ‚úÖ Ollama Container l√§uft?
3. ‚úÖ Model ist gepullt? (`ollama list`)
4. ‚úÖ Netzwerk-Konnektivit√§t? (beide im `coolify` network)
5. ‚úÖ Environment Variables korrekt?

---

## üìä Production Endpoints

### Public APIs

| Service | URL | Status |
|---------|-----|--------|
| Control Deck | https://control.brain.falklabs.de | ‚úÖ Live |
| AXE UI | https://axe.brain.falklabs.de | ‚úÖ Live |
| Backend API | https://api.brain.falklabs.de | ‚úÖ Live |
| AXE Chat API | https://api.brain.falklabs.de/api/axe/chat | ‚úÖ Live |

### Internal Services

| Service | Container | Network | Aliases |
|---------|-----------|---------|---------|
| Ollama | xkg0gc00sgcg0sc0g8wowskw-180855623729 | coolify | ollama, xkg0gc00... |
| AXEllm | axellm-mg400ss0o80gcs0owo0ckskc | coolify | axellm |
| Backend | vosss8wcg8cs80kcss8cgccc-193229452316 | coolify | vosss8wcg8... |

---

## üîê Security Notes

### Network Isolation

- ‚úÖ Alle Services im internen `coolify` Netzwerk
- ‚úÖ Ollama **nicht** √∂ffentlich exposed (nur intern via Docker-Netzwerk)
- ‚úÖ AXEllm **nicht** √∂ffentlich exposed
- ‚úÖ Nur Backend API ist √∂ffentlich (√ºber Traefik/Coolify Proxy)

### Authentication

- ‚úÖ AXE UI: Keine Auth (√∂ffentlicher Chat)
- ‚úÖ Control Deck: NextAuth.js (Login required)
- ‚úÖ Backend API: Je nach Endpoint (Skills, Missions, etc. gesch√ºtzt)

### Rate Limiting

- ‚úÖ Backend hat Rate Limiting aktiviert
- ‚úÖ AXE Chat: Subject to backend rate limits

---

## üìà Performance & Monitoring

### Resource Usage

```bash
# Check Ollama memory usage
docker stats xkg0gc00sgcg0sc0g8wowskw-180855623729 --no-stream

# Check AXEllm
docker stats axellm-mg400ss0o80gcs0owo0ckskc --no-stream

# Check Backend
docker stats vosss8wcg8cs80kcss8cgccc-193229452316 --no-stream
```

**Expected:**
- Ollama: ~500-800 MB RAM (mit geladenem Modell)
- AXEllm: ~50-100 MB RAM
- Backend: ~200-400 MB RAM

### Response Times

- Health Check: ~50-100ms
- Chat Request: ~1-3s (abh√§ngig von Prompt-L√§nge)

### Monitoring

- Uptime Kuma: https://uptimekuma-bgo8s400o00w80804okoc040.46.224.37.114.sslip.io:3001
- Coolify Dashboard: https://coolify.falklabs.de

---

## üîÑ Maintenance

### Model Updates

```bash
# Pull new model version
docker exec xkg0gc00sgcg0sc0g8wowskw-180855623729 \
  ollama pull qwen2.5:0.5b

# Remove old models
docker exec xkg0gc00sgcg0sc0g8wowskw-180855623729 \
  ollama rm <old-model-name>
```

### AXEllm Updates

```bash
# Via Coolify API
curl -X POST https://coolify.falklabs.de/api/v1/deploy \
  -H "Authorization: Bearer <token>" \
  -H "Content-Type: application/json" \
  -d '{"uuid":"mg400ss0o80gcs0owo0ckskc","force":true}'
```

### Backend Updates

Automatic via Coolify on git push to main branch.

---

## üêõ Known Issues & Limitations

### 1. Coolify Compose Storage

**Issue:** Coolify speichert docker-compose.yml in Datenbank, nicht aus Git

**Workaround:** √Ñnderungen via Coolify API (base64-encoded) pushen

**Example:**
```bash
ENCODED=$(base64 -w 0 /path/to/compose.yml)
curl -X PATCH https://coolify.falklabs.de/api/v1/services/<uuid> \
  -H "Authorization: Bearer <token>" \
  -d "{\"docker_compose_raw\": \"$ENCODED\"}"
```

### 2. Container Names vs Service Names

**Issue:** Docker DNS nutzt Container-Namen, nicht Service-Namen aus Coolify

**Solution:** Immer vollst√§ndige Container-Namen verwenden oder Netzwerk-Aliase

### 3. Model in Container-Namen

**Issue:** Test-Scripts m√ºssen mit richtigem Modellnamen (`qwen2.5:0.5b`) aufgerufen werden

**Solution:** Environment Variable `DEFAULT_MODEL` nutzen oder Modellname in Config

---

## üìù Changelog

### 2026-02-18 - Initial Deployment

**Commits:**
- `6ec3294` - fix(axe-stack): Configure AXEllm to use existing Ollama container
- `35c073b` - feat(axe): Add AXE Chat test script and Docker build helper

**Changes:**
- ‚úÖ AXE UI deployed to production
- ‚úÖ AXE Fusion module activated
- ‚úÖ AXEllm service configured
- ‚úÖ Ollama network alias setup
- ‚úÖ Test scripts added

**Result:** Full AXE Chat stack operational

---

## üéØ Success Criteria

- [x] AXE UI accessible at https://axe.brain.falklabs.de
- [x] Chat page functional
- [x] Backend API healthy
- [x] Ollama reachable from Backend
- [x] Ollama reachable from AXEllm
- [x] Chat requests return LLM responses
- [x] Response time < 5 seconds
- [x] No 503 errors
- [x] Single Ollama instance for all services

---

## üìû Support

**Issues?** Check:
1. Container logs: `docker logs <container-name>`
2. Health endpoints: `/health` und `/api/axe/health`
3. Network connectivity: `docker network inspect coolify`
4. Environment variables: `docker exec <container> env | grep OLLAMA`

**Documentation:**
- AXE Architecture: `/docs/AXE_ARCHITECTURE.md`
- Backend API: `https://api.brain.falklabs.de/docs`
- Coolify API: `https://coolify.io/docs/api`

---

**Last Updated:** 2026-02-18
**Maintained by:** FalkLabs DevOps Team
**Version:** 1.0.0
