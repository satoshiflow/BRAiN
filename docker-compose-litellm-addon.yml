# Add LiteLLM proxy service to docker-compose.yml
# Supports: OpenAI, Anthropic, Ollama, Azure, Cohere, etc.

  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: brain-litellm
    ports:
      - "4000:4000"
    environment:
      # OpenAI
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}

      # Anthropic Claude
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}

      # Local Ollama
      - OLLAMA_API_BASE=http://ollama:11434

      # Optional: Azure, Cohere, etc.
      # - AZURE_API_KEY=${AZURE_API_KEY:-}
      # - COHERE_API_KEY=${COHERE_API_KEY:-}
    volumes:
      - ./litellm_config.yaml:/app/config.yaml
    command: --config /app/config.yaml --port 4000
    depends_on:
      - ollama
    restart: unless-stopped
