# Docker Compose overrides for PRODUCTION environment
# Used with: docker-compose -f docker-compose.yml -f docker-compose.prod.yml up

services:
  backend:
    container_name: prod-backend
    # Ports 8000:8000 (default from base compose)
    environment:
      - APP_ENV=production
      - LOG_LEVEL=WARNING
      - ENABLE_API_CACHING=true

  control_deck:
    container_name: prod-control-deck
    # Ports 3000:3000 (default from base compose)
    environment:
      - NEXT_PUBLIC_BRAIN_API_BASE=https://brain.falklabs.de
      - NODE_ENV=production

  axe_ui:
    container_name: prod-axe-ui
    ports:
      - "3005:3000"  # Production AXE UI on port 3005
    environment:
      - NEXT_PUBLIC_BRAIN_API_BASE=https://brain.falklabs.de
      - NODE_ENV=production

  postgres:
    container_name: prod-postgres
    environment:
      POSTGRES_DB: brain_prod
    # Production should have regular backups configured
    # Consider adding backup volumes and backup service here

  redis:
    container_name: prod-redis
    # Production should have persistence enabled
    command: redis-server --appendonly yes
    volumes:
      - brain_redis_data:/data

  qdrant:
    container_name: prod-qdrant

  ollama:
    container_name: prod-ollama

  openwebui:
    container_name: prod-openwebui

  # ===== WORKER POOL (Phase 4) =====
  worker:
    build:
      context: ./backend
      dockerfile: Dockerfile.worker
    image: brain-worker:latest

    command: python worker.py

    environment:
      # Database
      - DATABASE_URL=${DATABASE_URL:-postgresql+asyncpg://brain:brain@postgres:5432/brain_prod}

      # Redis
      - REDIS_URL=redis://redis:6379/0

      # LLM
      - OLLAMA_HOST=http://ollama:11434
      - OLLAMA_MODEL=qwen2.5:0.5b

      # Vector DB
      - QDRANT_URL=http://qdrant:6333

      # Worker Config
      - WORKER_CONCURRENCY=2
      - LOG_LEVEL=INFO

    depends_on:
      - postgres
      - redis
      - ollama
      - qdrant

    networks:
      - brain_network

    # Horizontal Scaling - Start with 3 replicas
    deploy:
      mode: replicated
      replicas: 3

      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3

    # Health Check
    healthcheck:
      test: ["CMD", "python", "-c", "import redis; r=redis.from_url('redis://redis:6379'); r.ping()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  # ===== AXE STACK (Ollama + AXEllm) =====
  ollama:
    image: ollama/ollama:latest
    container_name: brain-ollama
    networks:
      - brain_network
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  axellm:
    build:
      context: ./services/axellm
      dockerfile: Dockerfile
    container_name: brain-axellm
    networks:
      - brain_network
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - DEFAULT_MODEL=qwen2.5:0.5b
      - REQUEST_TIMEOUT_SECONDS=60
      - LOG_LEVEL=INFO
    depends_on:
      ollama:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

volumes:
  brain_redis_data:  # Redis persistence for production
  ollama_data:  # Ollama models for AXE
