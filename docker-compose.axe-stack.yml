version: '3.8'

services:
  # ollama-qwen service - using existing Ollama container via service name 'ollama'
  # If you need to deploy Ollama separately, uncomment this:
  # ollama-qwen:
  #   image: ollama/ollama:latest
  #   container_name: ollama-qwen
  #   networks:
  #     - coolify
  #   volumes:
  #     - ollama-data:/root/.ollama
  #   restart: unless-stopped

  axellm:
    build:
      context: ./services/axellm
      dockerfile: Dockerfile
    container_name: axellm
    networks:
      - coolify
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - DEFAULT_MODEL=qwen2.5:0.5b
      - REQUEST_TIMEOUT_SECONDS=60
    restart: unless-stopped
    # Keine Ports exposed - nur intern Ã¼ber Docker Netzwerk erreichbar

networks:
  coolify:
    external: true
    name: coolify

# volumes:
#   ollama-data:  # Not needed when using existing Ollama container
